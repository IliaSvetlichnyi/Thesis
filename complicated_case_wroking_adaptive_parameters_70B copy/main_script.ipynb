{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the Graph with Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-31 23:10:42.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mFull API response: {'id': 'gen-rseWPwRQeyHK7RWQxV1Nw7kPjjpj', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1722453038, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the function definition for `step_10`:\\n```\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\ndef step_10(csv_path, SizeSegment):\\n    df = pd.read_csv(csv_path)\\n    signal = df['signal'].values\\n    Segments = [signal[i:i+SizeSegment] for i in range(0, len(signal), SizeSegment)]\\n    Segments_normalized = [MinMaxScaler().fit_transform(segment.reshape(-1, 1)).reshape(-1) for segment in Segments]\\n    return Segments_normalized\\n```\"}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 692, 'completion_tokens': 122, 'total_tokens': 814}}\u001b[0m\n",
      "\u001b[32m2024-07-31 23:10:45.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mFull API response: {'id': 'gen-wWcCls4nvDVFAEvhbJsm7P5kzdQJ', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1722453044, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the Python function `step_20`:\\n```\\nimport numpy as np\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\ndef step_20(Segments):\\n    Segments_normalized = []\\n    for segment in Segments:\\n        scaler = MinMaxScaler()\\n        Segments_normalized.append(scaler.fit_transform(segment.reshape(-1, 1)).flatten())\\n    return Segments_normalized\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'system_fingerprint': '601a0519fb4d41a706042f153a1732dce93cd158a93180a364be77ef4864bd39', 'usage': {'prompt_tokens': 696, 'completion_tokens': 80, 'total_tokens': 776}}\u001b[0m\n",
      "\u001b[32m2024-07-31 23:10:52.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mFull API response: {'id': 'gen-BNCacQN012yp1Oj73yDlctLMUXGv', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1722453047, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"```\\nimport pandas as pd\\nimport pywt\\n\\ndef step_30(Segments_normalized, Dec_levels):\\n    Features = []\\n    for segment in Segments_normalized:\\n        coeffs = pywt.wavedec(segment, 'db3', level=Dec_levels)\\n        features = [coefficient.mean() for coefficient in coeffs]\\n        Features.append(features)\\n    return Features\\n```\"}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 697, 'completion_tokens': 78, 'total_tokens': 775}}\u001b[0m\n",
      "\u001b[32m2024-07-31 23:11:02.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mFull API response: {'id': 'gen-stF5AecMTu7QRzGX9x6fMltUssF2', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1722453056, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the Python function `step_40`:\\n```\\nimport pandas as pd\\nfrom sklearn.decomposition import PCA\\n\\ndef step_40(Features, NC_pca):\\n    pca = PCA(n_components=NC_pca)\\n    PCA_Features = pca.fit_transform(Features)\\n    return PCA_Features, pca\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 693, 'completion_tokens': 67, 'total_tokens': 760}}\u001b[0m\n",
      "\u001b[32m2024-07-31 23:11:07.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mFull API response: {'id': 'gen-qnuFbwJaXwOfCWD0tuDB9ym017UR', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1722453064, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the Python function 'step_50' that meets the requirements:\\n```\\nimport numpy as np\\nfrom sklearn.svm import OneClassSVM\\nfrom sklearn.model_selection import train_test_split\\n\\ndef step_50(PCA_Features, kernel, nu, gamma):\\n    labels = np.ones(len(PCA_Features))\\n    X_train, X_test = train_test_split(PCA_Features, test_size=0.2, random_state=42)\\n    clf = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\\n    clf.fit(X_train)\\n    pred_train = clf.predict(X_train)\\n    err_train = np.mean(pred_train != 1)\\n    pred_test = clf.predict(X_test)\\n    err_test = np.mean(pred_test != -1)\\n    Prec_learn = 1 - err_train\\n    Prec_test = 1 - err_test\\n    return clf, Prec_learn, Prec_test\\n```\"}, 'finish_reason': 'stop', 'logprobs': None}], 'system_fingerprint': '601a0519fb4d41a706042f153a1732dce93cd158a93180a364be77ef4864bd39', 'usage': {'prompt_tokens': 824, 'completion_tokens': 192, 'total_tokens': 1016}}\u001b[0m\n",
      "\u001b[32m2024-07-31 23:11:08.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m320\u001b[0m - \u001b[1mValidation completed successfully.\u001b[0m\n",
      "\u001b[32m2024-07-31 23:11:08.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1mMain script validated successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configure logger\n",
    "logger.add(\"execution.log\", rotation=\"500 MB\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_url = \"https://openrouter.ai/api/v1\"\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "# Predefined parameters\n",
    "raw_data = pd.read_csv(\n",
    "    \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/datasets/complicated_case/learning-file_2.csv\")\n",
    "# Assume raw_data is a pandas DataFrame with 'timestamp' and 'signal' columns\n",
    "signal_data = raw_data['signal'].values\n",
    "\n",
    "# Adjust based on data size\n",
    "SizeSegment = min(512, len(signal_data) // 100)\n",
    "gamma = 'scale'  # Let sklearn choose an appropriate scale\n",
    "nu = 0.1  # This might need domain knowledge to set appropriately\n",
    "kernel = \"rbf\"  # This is often a good default\n",
    "\n",
    "# PCA\n",
    "# We'll use the signal data for PCA parameter calculation\n",
    "pca = PCA().fit(signal_data.reshape(-1, 1))\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "NC_pca = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "Dec_levels = int(np.log2(SizeSegment)) - 3  # Adjust based on segment size\n",
    "\n",
    "\n",
    "workflow_steps = {\n",
    "    10: {\n",
    "        \"description\": \"Import raw data from CSV and segment it\",\n",
    "        \"dependencies\": [],\n",
    "        \"input\": [\"csv_path\", \"SizeSegment\"],\n",
    "        \"output\": [\"Segments\"],\n",
    "        \"additional_info\": \"Use pandas to read the CSV and create segments of size SizeSegment.\"\n",
    "    },\n",
    "    20: {\n",
    "        \"description\": \"Normalize the segmented data using MinMaxScaler\",\n",
    "        \"dependencies\": [10],\n",
    "        \"input\": [\"Segments\"],\n",
    "        \"output\": [\"Segments_normalized\"],\n",
    "        \"additional_info\": \"Segments is a list of 1D numpy arrays. Each segment should be normalized independently.\"\n",
    "    },\n",
    "    30: {\n",
    "        \"description\": \"Extract features using wavelet decomposition\",\n",
    "        \"dependencies\": [20],\n",
    "        \"input\": [\"Segments_normalized\", \"Dec_levels\"],\n",
    "        \"output\": [\"Features\"],\n",
    "        \"additional_info\": \"Use pywavelets (pywt) library with 'db3' wavelet and specified Dec_levels.\"\n",
    "    },\n",
    "    40: {\n",
    "        \"description\": \"Apply PCA for dimension reduction\",\n",
    "        \"dependencies\": [30],\n",
    "        \"input\": [\"Features\", \"NC_pca\"],\n",
    "        \"output\": [\"PCA_Features\", \"pca\"],\n",
    "        \"additional_info\": \"Use sklearn's PCA. Return both the transformed features and the PCA object.\"\n",
    "    },\n",
    "    50: {\n",
    "        \"description\": \"Train model, evaluate, and calculate metrics\",\n",
    "        \"dependencies\": [40],\n",
    "        \"input\": [\"PCA_Features\", \"kernel\", \"nu\", \"gamma\"],\n",
    "        \"output\": [\"FittedClassifier\", \"Prec_learn\", \"Prec_test\"],\n",
    "        \"additional_info\": \"\"\"\n",
    "        1. Create labels: np.ones for learning data.\n",
    "        2. Split data into train and test sets (80% train, 20% test).\n",
    "        3. Create and fit a One-Class SVM classifier using sklearn.\n",
    "        4. Predict labels for training data.\n",
    "        5. Calculate error rate for training data.\n",
    "        6. Predict labels for test data (assume all test data as anomaly, i.e., -1).\n",
    "        7. Calculate error rate for test data.\n",
    "        8. Calculate precision as 1 - error_rate for both training and test.\n",
    "        Return the fitted classifier and both precision values.\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example step and validation scripts to guide the model\n",
    "example_step_script = \"\"\"\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def step_40(Segments_normalized, Dec_levels):\n",
    "    Features = []\n",
    "    for segment in Segments_normalized:\n",
    "        coeffs = pywt.wavedec(segment, 'db4', level=Dec_levels)\n",
    "        features = [coefficient.mean() for coefficient in coeffs]\n",
    "        Features.append(features)\n",
    "    return StandardScaler().fit_transform(Features)\n",
    "\"\"\"\n",
    "\n",
    "example_validation_script = \"\"\"\n",
    "import pandas as pd\n",
    "from step_10 import step_10\n",
    "from step_20 import step_20\n",
    "from step_30 import step_30\n",
    "from step_40 import step_40\n",
    "\n",
    "def validate_step():\n",
    "    csv_path = '/path/to/your/csv/file.csv'\n",
    "    raw_data = step_10(csv_path)\n",
    "    Segments = step_20(raw_data, SizeSegment=512)\n",
    "    Segments_normalized = step_30(Segments)\n",
    "    Features = step_40(Segments_normalized, Dec_levels=5)\n",
    "    print(Features)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    validate_step()\n",
    "\"\"\"\n",
    "\n",
    "def openai_chat(request):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"meta-llama/llama-3-70b-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": request}]\n",
    "    }\n",
    "    response = requests.post(f\"{api_url}/chat/completions\", headers=headers, json=data)\n",
    "    \n",
    "    # Log the full response for debugging\n",
    "    response_json = response.json()\n",
    "    logger.info(f\"Full API response: {response_json}\")\n",
    "    \n",
    "    # Check if 'choices' key exists in the response\n",
    "    if \"choices\" in response_json and response_json[\"choices\"]:\n",
    "        return response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        logger.error(f\"Request: {data}\")\n",
    "        logger.error(f\"Response: {response_json}\")\n",
    "        raise ValueError(\"The response does not contain 'choices'. Full response: \" + str(response_json))\n",
    "\n",
    "def generate_code_snippet(request):\n",
    "    response = openai_chat(request)\n",
    "    return response\n",
    "\n",
    "def clean_and_correct_code(generated_code, csv_path):\n",
    "    cleaned_code = generated_code.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    cleaned_code_lines = [line for line in cleaned_code.split(\"\\n\") if not line.lower().startswith(\"here is the\")]\n",
    "    cleaned_code = \"\\n\".join(cleaned_code_lines)\n",
    "    corrected_code = cleaned_code.replace(\"{csv_path}\", f\"'{csv_path}'\")\n",
    "    return corrected_code\n",
    "\n",
    "def validate_unit_code(code_filename):\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", code_filename], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(result.stderr)\n",
    "        return True, result.stdout\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def fix_code(code_snippet, error_message, csv_path):\n",
    "    request = (\n",
    "        f\"The following code snippet encountered an error:\\n\\n{code_snippet}\\n\\n\"\n",
    "        f\"Error message:\\n{error_message}\\n\\n\"\n",
    "        f\"Please fix the code snippet to resolve the error without providing any explanations or comments.\"\n",
    "    )\n",
    "    fixed_code = generate_code_snippet(request)\n",
    "    return clean_and_correct_code(fixed_code, csv_path)\n",
    "\n",
    "def get_all_dependencies(step, workflow_steps):\n",
    "    dependencies = set(workflow_steps[step][\"dependencies\"])\n",
    "    for dep in workflow_steps[step][\"dependencies\"]:\n",
    "        dependencies.update(get_all_dependencies(dep, workflow_steps))\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "def generate_code_for_step(step, workflow_steps, csv_path, dataset_info):\n",
    "    additional_info = workflow_steps[step].get(\"additional_info\", \"\")\n",
    "    request = (\n",
    "        f\"Here is an example of a good step script:\\n\\n{example_step_script}\\n\\n\"\n",
    "        f\"Write a Python function named 'step_{step}' for the following step: {workflow_steps[step]['description']}. \"\n",
    "        f\"The function should take {', '.join(workflow_steps[step]['input'])} as input and return {', '.join(workflow_steps[step]['output'])}. \"\n",
    "        f\"Ensure to include necessary imports and handle edge cases. \"\n",
    "        f\"Additional information: {additional_info}\\n\"\n",
    "        f\"The dataset has the following columns: {dataset_info['columns']}. \"\n",
    "        f\"The data types are: {dataset_info['types']}. \"\n",
    "        f\"Here's a sample of the data: {dataset_info['sample_data']}. \"\n",
    "        f\"Value counts (top 5): {dataset_info['value_counts']}. \"\n",
    "        f\"Statistical description: {dataset_info['description']}. \"\n",
    "        f\"Use these predefined parameters if needed: SizeSegment={SizeSegment}, gamma={gamma}, nu={nu}, kernel='{kernel}', NC_pca={NC_pca}, Dec_levels={Dec_levels}. \"\n",
    "        f\"The input 'Segments' is a list of 1D numpy arrays, each representing a segment of the signal data. \"\n",
    "        f\"Each segment should be normalized independently using sklearn's MinMaxScaler. \"\n",
    "        f\"The output 'Segments_normalized' should be a list of normalized 1D numpy arrays. \"\n",
    "        f\"Only return the function definition without any additional code or explanations.\"\n",
    "    )\n",
    "    code_snippet = generate_code_snippet(request)\n",
    "    return clean_and_correct_code(code_snippet, csv_path)\n",
    "\n",
    "def generate_validation_file(step, workflow_steps):\n",
    "    dependencies = get_all_dependencies(step, workflow_steps)\n",
    "    input_params = workflow_steps[step][\"input\"]\n",
    "    output_params = workflow_steps[step][\"output\"]\n",
    "\n",
    "    validation_code = \"import pandas as pd\\n\"\n",
    "    for dep in sorted(dependencies):\n",
    "        validation_code += f\"from step_{dep} import step_{dep}\\n\"\n",
    "    validation_code += f\"from step_{step} import step_{step}\\n\\n\"\n",
    "    \n",
    "    # Add predefined parameters\n",
    "    validation_code += f\"SizeSegment = {SizeSegment}\\n\"\n",
    "    validation_code += f\"gamma = '{gamma}'\\n\"\n",
    "    validation_code += f\"nu = {nu}\\n\"\n",
    "    validation_code += f\"kernel = '{kernel}'\\n\"\n",
    "    validation_code += f\"NC_pca = {NC_pca}\\n\"\n",
    "    validation_code += f\"Dec_levels = {Dec_levels}\\n\\n\"\n",
    "\n",
    "    validation_code += \"def validate_step():\\n\"\n",
    "    validation_code += \"    csv_path = '/Users/ilya/Desktop/GitHub_Repositories/Thesis/datasets/complicated_case/learning-file_2.csv'\\n\"\n",
    "\n",
    "    for dep in sorted(dependencies):\n",
    "        dep_inputs = \", \".join(workflow_steps[dep][\"input\"])\n",
    "        dep_outputs = \", \".join(workflow_steps[dep][\"output\"])\n",
    "        validation_code += f\"    {dep_outputs} = step_{dep}({dep_inputs})\\n\"\n",
    "\n",
    "    input_values = \", \".join(input_params)\n",
    "    output_values = \", \".join(output_params)\n",
    "    validation_code += f\"    {output_values} = step_{step}({input_values})\\n\"\n",
    "    validation_code += f\"    print({output_values})\\n\"\n",
    "\n",
    "    validation_code += \"\\nif __name__ == '__main__':\\n\"\n",
    "    validation_code += \"    validate_step()\\n\"\n",
    "\n",
    "    with open(f\"validate_step_{step}.py\", \"w\") as file:\n",
    "        file.write(validation_code)\n",
    "\n",
    "def save_dataset_info(csv_path, info_file_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    columns = df.columns.tolist()\n",
    "    types = df.dtypes.apply(lambda x: str(x)).to_dict()\n",
    "    sample_data = df.head().to_dict(orient='list')\n",
    "    value_counts = {col: df[col].value_counts().head().to_dict() for col in df.columns}\n",
    "    description = df.describe().to_dict()\n",
    "\n",
    "    dataset_info = {\n",
    "        'columns': columns,\n",
    "        'types': types,\n",
    "        'sample_data': sample_data,\n",
    "        'value_counts': value_counts,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "    with open(info_file_path, 'w') as file:\n",
    "        json.dump(dataset_info, file)\n",
    "\n",
    "def generate_main_file(workflow_steps, selected_step_numbers, csv_path):\n",
    "    main_code = \"import pandas as pd\\n\\n\"\n",
    "    for step in selected_step_numbers:\n",
    "        main_code += f\"from step_{step} import step_{step}\\n\"\n",
    "    \n",
    "    # Add predefined parameters\n",
    "    main_code += f\"\\nSizeSegment = {SizeSegment}\\n\"\n",
    "    main_code += f\"gamma = '{gamma}'\\n\"\n",
    "    main_code += f\"nu = {nu}\\n\"\n",
    "    main_code += f\"kernel = '{kernel}'\\n\"\n",
    "    main_code += f\"NC_pca = {NC_pca}\\n\"\n",
    "    main_code += f\"Dec_levels = {Dec_levels}\\n\\n\"\n",
    "\n",
    "    main_code += \"def main():\\n\"\n",
    "    main_code += f\"    csv_path = '{csv_path}'\\n\"\n",
    "\n",
    "    for step in selected_step_numbers:\n",
    "        input_params = \", \".join(workflow_steps[step]['input'])\n",
    "        output_params = \", \".join(workflow_steps[step]['output'])\n",
    "        main_code += f\"    {output_params} = step_{step}({input_params})\\n\"\n",
    "\n",
    "    main_code += \"    print(f'Precision on training data: {Prec_learn:.2f}')\\n\"\n",
    "    main_code += \"    print(f'Precision on test data: {Prec_test:.2f}')\\n\"\n",
    "\n",
    "    main_code += \"\\nif __name__ == '__main__':\\n\"\n",
    "    main_code += \"    main()\"\n",
    "\n",
    "    with open(\"main.py\", \"w\") as file:\n",
    "        file.write(main_code)\n",
    "\n",
    "def main():\n",
    "    csv_path = \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/datasets/complicated_case/learning-file_2.csv\"\n",
    "    info_file_path = \"dataset_info.json\"\n",
    "\n",
    "    if not os.path.exists(info_file_path):\n",
    "        save_dataset_info(csv_path, info_file_path)\n",
    "\n",
    "    with open(info_file_path, 'r') as f:\n",
    "        dataset_info = json.load(f)\n",
    "\n",
    "    selected_step_numbers = [10, 20, 30, 40, 50]\n",
    "    for step in selected_step_numbers:\n",
    "        try:\n",
    "            code_snippet = generate_code_for_step(step, workflow_steps, csv_path, dataset_info)\n",
    "            with open(f\"step_{step}.py\", \"w\") as file:\n",
    "                file.write(code_snippet)\n",
    "            generate_validation_file(step, workflow_steps)\n",
    "\n",
    "            success, output = validate_unit_code(f\"validate_step_{step}.py\")\n",
    "            while not success:\n",
    "                logger.info(f\"Validation failed for step {step}: {output}\")\n",
    "                fixed_code = fix_code(code_snippet, output, csv_path)\n",
    "                with open(f\"step_{step}.py\", \"w\") as file:\n",
    "                    file.write(fixed_code)\n",
    "                generate_validation_file(step, workflow_steps)\n",
    "                success, output = validate_unit_code(f\"validate_step_{step}.py\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing step {step}: {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(\"Validation completed successfully.\")\n",
    "\n",
    "    generate_main_file(workflow_steps, selected_step_numbers, csv_path)\n",
    "\n",
    "    # Validate the main script\n",
    "    success, output = validate_unit_code(\"main.py\")\n",
    "    if success:\n",
    "        logger.info(\"Main script validated successfully.\")\n",
    "    else:\n",
    "        logger.info(f\"Validation failed for main script.\")\n",
    "        logger.info(f\"Error: {output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

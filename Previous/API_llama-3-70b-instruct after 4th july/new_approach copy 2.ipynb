{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the Graph with Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_steps = {\n",
    "    11: {\n",
    "        \"description\": \"Load the CSV file as pandas DataFrame\",\n",
    "        \"dependencies\": [],\n",
    "        \"input\": [\"csv_path\"],\n",
    "        \"output\": [\"df\"]\n",
    "    },\n",
    "    21: {\n",
    "        \"description\": \"Examine the structure and characteristics of the data\",\n",
    "        \"dependencies\": [11],\n",
    "        \"input\": [\"df\"],\n",
    "        \"output\": [\"structure_info\"]\n",
    "    },\n",
    "    31: {\n",
    "        \"description\": \"Identify missing values, data types, and handle missing values if there are any\",\n",
    "        \"dependencies\": [11, 21],\n",
    "        \"input\": [\"df\"],\n",
    "        \"output\": [\"df_cleaned\", \"data_types_info\"]\n",
    "    },\n",
    "    32: {\n",
    "        \"description\": \"Identify if there is a need to convert categorical variables to numerical representations. If yes, then convert them.\",\n",
    "        \"dependencies\": [11, 31],\n",
    "        \"input\": [\"df_cleaned\", \"data_types_info\"],\n",
    "        \"output\": [\"df_encoded\"]\n",
    "    },\n",
    "    51: {\n",
    "        \"description\": \"Split the preprocessed data into training and testing sets, and implement a machine learning algorithm (choose from scikit-learn, XGBoost, LightGBM, or CatBoost).\",\n",
    "        \"dependencies\": [11, 31, 32],\n",
    "        \"input\": [\"df_encoded\"],\n",
    "        \"output\": [\"model\", \"X_train\", \"X_test\", \"y_train\", \"y_test\"]\n",
    "    },\n",
    "    61: {\n",
    "        \"description\": \"Evaluate the model's performance on both training and testing data, calculate evaluation metrics (for classification: [accuracy, precision, recall, F1-score]; for regression: [R^2, MSE, RMSE]), and compare the difference.\",\n",
    "        \"dependencies\": [51],\n",
    "        \"input\": [\"model\", \"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n",
    "        \"output\": [\"evaluation_results\", \"metrics\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Validation Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "\n",
    "# Configure logger\n",
    "logger.add(\"execution.log\", rotation=\"500 MB\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_url = \"https://openrouter.ai/api/v1\"\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "# Example step and validation scripts to guide the model\n",
    "example_step_script = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def step_32(df_cleaned, data_types_info):\n",
    "    df_encoded = df_cleaned.copy()\n",
    "    categorical_cols = [col for col, dtype in data_types_info.items() if dtype == 'object']\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "    return df_encoded\n",
    "\"\"\"\n",
    "\n",
    "example_validation_script = \"\"\"\n",
    "import pandas as pd\n",
    "from step_11 import step_11\n",
    "from step_31 import step_31\n",
    "from step_32 import step_32\n",
    "\n",
    "def validate_step():\n",
    "    csv_path = '/Users/ilya/Desktop/GitHub_Repositories/HW_University/Data_Mining/datasets/insurance.csv'\n",
    "    df = step_11(csv_path)\n",
    "    df_cleaned, data_types_info = step_31(df)\n",
    "    df_encoded = step_32(df_cleaned, data_types_info)\n",
    "    logger.info(df_encoded)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    validate_step()\n",
    "\"\"\"\n",
    "\n",
    "def openai_chat(request):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"meta-llama/llama-3-70b-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": request}]\n",
    "    }\n",
    "    response = requests.post(f\"{api_url}/chat/completions\", headers=headers, json=data)\n",
    "    \n",
    "    # Log the full response for debugging\n",
    "    response_json = response.json()\n",
    "    logger.debug(f\"Full API response: {response_json}\")\n",
    "    \n",
    "    # Check if 'choices' key exists in the response\n",
    "    if \"choices\" in response_json and response_json[\"choices\"]:\n",
    "        return response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        logger.error(f\"Request: {data}\")\n",
    "        logger.error(f\"Response: {response_json}\")\n",
    "        raise ValueError(\"The response does not contain 'choices'. Full response: \" + str(response_json))\n",
    "\n",
    "def generate_code_snippet(request):\n",
    "    response = openai_chat(request)\n",
    "    return response\n",
    "\n",
    "def clean_and_correct_code(generated_code, csv_path):\n",
    "    cleaned_code = generated_code.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    cleaned_code_lines = [line for line in cleaned_code.split(\"\\n\") if not line.lower().startswith(\"here is the\")]\n",
    "    cleaned_code = \"\\n\".join(cleaned_code_lines)\n",
    "    corrected_code = cleaned_code.replace(\"{csv_path}\", f\"'{csv_path}'\")\n",
    "    return corrected_code\n",
    "\n",
    "def validate_unit_code(code_filename):\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", code_filename], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(result.stderr)\n",
    "        return True, result.stdout\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def fix_code(code_snippet, error_message, csv_path):\n",
    "    request = (\n",
    "        f\"The following code snippet encountered an error:\\n\\n{code_snippet}\\n\\n\"\n",
    "        f\"Error message:\\n{error_message}\\n\\n\"\n",
    "        f\"Please fix the code snippet to resolve the error without providing any explanations or comments.\"\n",
    "    )\n",
    "    fixed_code = generate_code_snippet(request)\n",
    "    return clean_and_correct_code(fixed_code, csv_path)\n",
    "\n",
    "def get_all_dependencies(step, workflow_steps):\n",
    "    dependencies = set(workflow_steps[step][\"dependencies\"])\n",
    "    for dep in workflow_steps[step][\"dependencies\"]:\n",
    "        dependencies.update(get_all_dependencies(dep, workflow_steps))\n",
    "    return dependencies\n",
    "\n",
    "def generate_code_for_step(step, workflow_steps, csv_path, dataset_info):\n",
    "    request = (\n",
    "        f\"Here is an example of a good step script:\\n\\n{example_step_script}\\n\\n\"\n",
    "        f\"Write a Python function named 'step_{step}' for the following step: {workflow_steps[step]['description']}. \"\n",
    "        f\"The function should take {', '.join(workflow_steps[step]['input'])} as input and return {', '.join(workflow_steps[step]['output'])}. \"\n",
    "        f\"Ensure to include necessary imports and handle edge cases. \"\n",
    "        f\"The dataset has the following columns: {dataset_info['columns']}. \"\n",
    "        f\"The data types are: {dataset_info['types']}. \"\n",
    "        f\"Here's a sample of the data: {dataset_info['sample_data']}. \"\n",
    "        f\"Value counts (top 5): {dataset_info['value_counts']}. \"\n",
    "        f\"Statistical description: {dataset_info['description']}. \"\n",
    "        f\"Only return the function definition without any additional code or explanations.\"\n",
    "    )\n",
    "    code_snippet = generate_code_snippet(request)\n",
    "    logger.debug(f\"Generated code snippet for step {step}\")\n",
    "    return clean_and_correct_code(code_snippet, csv_path)\n",
    "\n",
    "def generate_validation_file(step, workflow_steps):\n",
    "    dependencies = get_all_dependencies(step, workflow_steps)\n",
    "    input_params = workflow_steps[step][\"input\"]\n",
    "    output_params = workflow_steps[step][\"output\"]\n",
    "\n",
    "    validation_code = \"import pandas as pd\\n\"\n",
    "    for dep in sorted(dependencies):\n",
    "        validation_code += f\"from step_{dep} import step_{dep}\\n\"\n",
    "    validation_code += f\"from step_{step} import step_{step}\\n\\n\"\n",
    "\n",
    "    validation_code += \"def validate_step():\\n\"\n",
    "    validation_code += \"    csv_path = '/Users/ilya/Desktop/GitHub_Repositories/Thesis/datasets/insurance.csv'\\n\"\n",
    "\n",
    "    for dep in sorted(dependencies):\n",
    "        dep_inputs = \", \".join(workflow_steps[dep][\"input\"])\n",
    "        dep_outputs = \", \".join(workflow_steps[dep][\"output\"])\n",
    "        validation_code += f\"    {dep_outputs} = step_{dep}({dep_inputs})\\n\"\n",
    "\n",
    "    input_values = \", \".join(input_params)\n",
    "    output_values = \", \".join(output_params)\n",
    "    validation_code += f\"    {output_values} = step_{step}({input_values})\\n\"\n",
    "    validation_code += f\"    logger.info({output_values})\\n\"\n",
    "\n",
    "    validation_code += \"\\nif __name__ == '__main__':\\n\"\n",
    "    validation_code += \"    validate_step()\\n\"\n",
    "\n",
    "    with open(f\"validate_step_{step}.py\", \"w\") as file:\n",
    "        file.write(validation_code)\n",
    "\n",
    "def save_dataset_info(csv_path, info_file_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    columns = df.columns.tolist()\n",
    "    types = df.dtypes.apply(lambda x: str(x)).to_dict()\n",
    "    sample_data = df.head().to_dict(orient='list')\n",
    "    value_counts = {col: df[col].value_counts().head().to_dict() for col in df.columns}\n",
    "    description = df.describe().to_dict()\n",
    "\n",
    "    dataset_info = {\n",
    "        'columns': columns,\n",
    "        'types': types,\n",
    "        'sample_data': sample_data,\n",
    "        'value_counts': value_counts,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "    with open(info_file_path, 'w') as file:\n",
    "        json.dump(dataset_info, file)\n",
    "\n",
    "def main():\n",
    "    csv_path = \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/datasets/insurance.csv\"\n",
    "    info_file_path = \"dataset_info.json\"\n",
    "\n",
    "    if not os.path.exists(info_file_path):\n",
    "        save_dataset_info(csv_path, info_file_path)\n",
    "\n",
    "    with open(info_file_path, 'r') as f:\n",
    "        dataset_info = json.load(f)\n",
    "\n",
    "    selected_step_numbers = [11, 21, 31, 32, 51, 61]\n",
    "    for step in selected_step_numbers:\n",
    "        try:\n",
    "            code_snippet = generate_code_for_step(step, workflow_steps, csv_path, dataset_info)\n",
    "            with open(f\"step_{step}.py\", \"w\") as file:\n",
    "                file.write(code_snippet)\n",
    "            generate_validation_file(step, workflow_steps)\n",
    "\n",
    "            success, output = validate_unit_code(f\"validate_step_{step}.py\")\n",
    "            while not success:\n",
    "                logger.error(f\"Validation failed for step {step}: {output}\")\n",
    "                fixed_code = fix_code(code_snippet, output, csv_path)\n",
    "                with open(f\"step_{step}.py\", \"w\") as file:\n",
    "                    file.write(fixed_code)\n",
    "                generate_validation_file(step, workflow_steps)\n",
    "                success, output = validate_unit_code(f\"validate_step_{step}.py\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing step {step}: {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(\"Validation completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the Main Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-18 21:36:29.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mFull API response: {'id': 'gen-4syLHYrwF5PLN02xjru5bpEPObEa', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324174, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```\\nimport pandas as pd\\n\\ndef step_11(csv_path):\\n    try:\\n        df = pd.read_csv(csv_path)\\n        if not set(df.columns) == set([\\'age\\', \\'sex\\', \\'bmi\\', \\'children\\', \\'smoker\\', \\'region\\', \\'charges\\']):\\n            raise ValueError(\"Columns do not match the expected dataset\")\\n        data_types_info = {\\'age\\': \\'int64\\', \\'sex\\': \\'object\\', \\'bmi\\': \\'float64\\', \\'children\\': \\'int64\\', \\'smoker\\': \\'object\\', \\'region\\': \\'object\\', \\'charges\\': \\'float64\\'}\\n        if not df.dtypes.apply(lambda x: x.name).to_dict() == data_types_info:\\n            raise ValueError(\"Data types do not match the expected dataset\")\\n        return df\\n    except FileNotFoundError:\\n        print(\"The file at the specified path was not found\")\\n        return None\\n    except pd.errors.EmptyDataError:\\n        print(\"The file at the specified path is empty\")\\n        return None\\n    except pd.errors.ParserError:\\n        print(\"Error parsing the file at the specified path\")\\n        return None\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 983, 'completion_tokens': 230, 'total_tokens': 1213}}\u001b[0m\n",
      "\u001b[32m2024-07-18 21:36:29.516\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_code_for_step\u001b[0m:\u001b[36m121\u001b[0m - \u001b[34m\u001b[1mGenerated code snippet for step 11\u001b[0m\n",
      "\u001b[32m2024-07-18 21:36:29.809\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m192\u001b[0m - \u001b[31m\u001b[1mValidation failed for step 11: Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 10, in <module>\n",
      "    validate_step()\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 7, in validate_step\n",
      "    logger.info(df)\n",
      "NameError: name 'logger' is not defined\n",
      "\u001b[0m\n",
      "\u001b[32m2024-07-18 21:36:48.586\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mFull API response: {'id': 'gen-8maz62LYTzMKhAcEjgy24zTCds6w', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324191, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the corrected code snippet:\\n\\n```\\nimport pandas as pd\\nimport logging\\n\\nlogger = logging.getLogger()\\n\\ndef step_11(csv_path):\\n    try:\\n        df = pd.read_csv(csv_path)\\n        if not set(df.columns) == set([\\'age\\', \\'sex\\', \\'bmi\\', \\'children\\', \\'smoker\\', \\'region\\', \\'charges\\']):\\n            raise ValueError(\"Columns do not match the expected dataset\")\\n        data_types_info = {\\'age\\': \\'int64\\', \\'sex\\': \\'object\\', \\'bmi\\': \\'float64\\', \\'children\\': \\'int64\\', \\'smoker\\': \\'object\\', \\'region\\': \\'object\\', \\'charges\\': \\'float64\\'}\\n        if not df.dtypes.apply(lambda x: x.name).to_dict() == data_types_info:\\n            raise ValueError(\"Data types do not match the expected dataset\")\\n        logger.info(df)\\n        return df\\n    except FileNotFoundError:\\n        logger.info(\"The file at the specified path was not found\")\\n        return None\\n    except pd.errors.EmptyDataError:\\n        logger.info(\"The file at the specified path is empty\")\\n        return None\\n    except pd.errors.ParserError:\\n        logger.info(\"Error parsing the file at the specified path\")\\n        return None\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 386, 'completion_tokens': 253, 'total_tokens': 639}}\u001b[0m\n",
      "\u001b[32m2024-07-18 21:36:48.951\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m192\u001b[0m - \u001b[31m\u001b[1mValidation failed for step 11: Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 10, in <module>\n",
      "    validate_step()\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 7, in validate_step\n",
      "    logger.info(df)\n",
      "NameError: name 'logger' is not defined\n",
      "\u001b[0m\n",
      "\u001b[32m2024-07-18 21:36:53.169\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mFull API response: {'id': 'gen-c2pMMTMUxjaT0PU2dFQlSGVe9AfO', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324209, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the fixed code snippet:\\n\\n```\\nimport pandas as pd\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\ndef step_11(csv_path):\\n    try:\\n        df = pd.read_csv(csv_path)\\n        if not set(df.columns) == set([\\'age\\', \\'sex\\', \\'bmi\\', \\'children\\', \\'smoker\\', \\'region\\', \\'charges\\']):\\n            raise ValueError(\"Columns do not match the expected dataset\")\\n        data_types_info = {\\'age\\': \\'int64\\', \\'sex\\': \\'object\\', \\'bmi\\': \\'float64\\', \\'children\\': \\'int64\\', \\'smoker\\': \\'object\\', \\'region\\': \\'object\\', \\'charges\\': \\'float64\\'}\\n        if not df.dtypes.apply(lambda x: x.name).to_dict() == data_types_info:\\n            raise ValueError(\"Data types do not match the expected dataset\")\\n        logger.info(df)\\n        return df\\n    except FileNotFoundError:\\n        logger.error(\"The file at the specified path was not found\")\\n        return None\\n    except pd.errors.EmptyDataError:\\n        logger.error(\"The file at the specified path is empty\")\\n        return None\\n    except pd.errors.ParserError:\\n        logger.error(\"Error parsing the file at the specified path\")\\n        return None\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 386, 'completion_tokens': 262, 'total_tokens': 648}}\u001b[0m\n",
      "\u001b[32m2024-07-18 21:36:53.489\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m192\u001b[0m - \u001b[31m\u001b[1mValidation failed for step 11: INFO:step_11:      age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 10, in <module>\n",
      "    validate_step()\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 7, in validate_step\n",
      "    logger.info(df)\n",
      "NameError: name 'logger' is not defined\n",
      "\u001b[0m\n",
      "\u001b[32m2024-07-18 21:37:01.999\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mFull API response: {'id': 'gen-UbFcv3BQPVOt7dxtf0hucaS6a3pj', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324215, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```\\nimport pandas as pd\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger()\\n\\ndef step_11(csv_path):\\n    try:\\n        df = pd.read_csv(csv_path)\\n        if not set(df.columns) == set([\\'age\\', \\'sex\\', \\'bmi\\', \\'children\\', \\'smoker\\', \\'region\\', \\'charges\\']):\\n            raise ValueError(\"Columns do not match the expected dataset\")\\n        data_types_info = {\\'age\\': \\'int64\\', \\'sex\\': \\'object\\', \\'bmi\\': \\'float64\\', \\'children\\': \\'int64\\', \\'smoker\\': \\'object\\', \\'region\\': \\'object\\', \\'charges\\': \\'float64\\'}\\n        if not df.dtypes.apply(lambda x: x.name).to_dict() == data_types_info:\\n            raise ValueError(\"Data types do not match the expected dataset\")\\n        logger.info(df)\\n        return df\\n    except FileNotFoundError:\\n        logger.info(\"The file at the specified path was not found\")\\n        return None\\n    except pd.errors.EmptyDataError:\\n        logger.info(\"The file at the specified path is empty\")\\n        return None\\n    except pd.errors.ParserError:\\n        logger.info(\"Error parsing the file at the specified path\")\\n        return None\\n```'}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 695, 'completion_tokens': 253, 'total_tokens': 948}}\u001b[0m\n",
      "\u001b[32m2024-07-18 21:37:02.313\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m192\u001b[0m - \u001b[31m\u001b[1mValidation failed for step 11: INFO:root:      age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 10, in <module>\n",
      "    validate_step()\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 7, in validate_step\n",
      "    logger.info(df)\n",
      "NameError: name 'logger' is not defined\n",
      "\u001b[0m\n",
      "\u001b[32m2024-07-18 21:37:10.705\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mFull API response: {'id': 'gen-6PC6y47biFuVisyPGumKPyCZsZZp', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324223, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the corrected code:\\n\\n```\\nimport pandas as pd\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger()\\n\\ndef step_11(csv_path):\\n    try:\\n        df = pd.read_csv(csv_path)\\n        if not set(df.columns) == set([\\'age\\', \\'sex\\', \\'bmi\\', \\'children\\', \\'smoker\\', \\'region\\', \\'charges\\']):\\n            raise ValueError(\"Columns do not match the expected dataset\")\\n        data_types_info = {\\'age\\': \\'int64\\', \\'sex\\': \\'object\\', \\'bmi\\': \\'float64\\', \\'children\\': \\'int64\\', \\'smoker\\': \\'object\\', \\'region\\': \\'object\\', \\'charges\\': \\'float64\\'}\\n        if not df.dtypes.apply(lambda x: x.name).to_dict() == data_types_info:\\n            raise ValueError(\"Data types do not match the expected dataset\")\\n        logger.info(df)\\n        return df\\n    except FileNotFoundError:\\n        logger.info(\"The file at the specified path was not found\")\\n        return None\\n    except pd.errors.EmptyDataError:\\n        logger.info(\"The file at the specified path is empty\")\\n        return None\\n    except pd.errors.ParserError:\\n        logger.info(\"Error parsing the file at the specified path\")\\n        return None\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 693, 'completion_tokens': 259, 'total_tokens': 952}}\u001b[0m\n",
      "\u001b[32m2024-07-18 21:37:11.018\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m192\u001b[0m - \u001b[31m\u001b[1mValidation failed for step 11: INFO:root:      age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 10, in <module>\n",
      "    validate_step()\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 7, in validate_step\n",
      "    logger.info(df)\n",
      "NameError: name 'logger' is not defined\n",
      "\u001b[0m\n",
      "\u001b[32m2024-07-18 21:37:20.044\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mFull API response: {'id': 'gen-1pT3lNOdndlJVIWIeS3JoZtyNDjO', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324232, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the corrected code snippet:\\n\\n```\\nimport pandas as pd\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\ndef step_11(csv_path):\\n    try:\\n        df = pd.read_csv(csv_path)\\n        if not set(df.columns) == set([\\'age\\', \\'sex\\', \\'bmi\\', \\'children\\', \\'smoker\\', \\'region\\', \\'charges\\']):\\n            raise ValueError(\"Columns do not match the expected dataset\")\\n        data_types_info = {\\'age\\': \\'int64\\', \\'sex\\': \\'object\\', \\'bmi\\': \\'float64\\', \\'children\\': \\'int64\\', \\'smoker\\': \\'object\\', \\'region\\': \\'object\\', \\'charges\\': \\'float64\\'}\\n        if not df.dtypes.apply(lambda x: x.name).to_dict() == data_types_info:\\n            raise ValueError(\"Data types do not match the expected dataset\")\\n        logger.info(df)\\n        return df\\n    except FileNotFoundError:\\n        logger.info(\"The file at the specified path was not found\")\\n        return None\\n    except pd.errors.EmptyDataError:\\n        logger.info(\"The file at the specified path is empty\")\\n        return None\\n    except pd.errors.ParserError:\\n        logger.info(\"Error parsing the file at the specified path\")\\n        return None\\n```'}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 693, 'completion_tokens': 262, 'total_tokens': 955}}\u001b[0m\n",
      "\u001b[32m2024-07-18 21:37:20.399\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m192\u001b[0m - \u001b[31m\u001b[1mValidation failed for step 11: INFO:step_11:      age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 10, in <module>\n",
      "    validate_step()\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 7, in validate_step\n",
      "    logger.info(df)\n",
      "NameError: name 'logger' is not defined\n",
      "\u001b[0m\n",
      "\u001b[32m2024-07-18 21:37:28.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mFull API response: {'id': 'gen-vln5IxyOdAWbr6kYETa028twRNa2', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324241, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the corrected code:\\n\\n```\\nimport pandas as pd\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger()\\n\\ndef step_11(csv_path):\\n    try:\\n        df = pd.read_csv(csv_path)\\n        if not set(df.columns) == set([\\'age\\', \\'sex\\', \\'bmi\\', \\'children\\', \\'smoker\\', \\'region\\', \\'charges\\']):\\n            raise ValueError(\"Columns do not match the expected dataset\")\\n        data_types_info = {\\'age\\': \\'int64\\', \\'sex\\': \\'object\\', \\'bmi\\': \\'float64\\', \\'children\\': \\'int64\\', \\'smoker\\': \\'object\\', \\'region\\': \\'object\\', \\'charges\\': \\'float64\\'}\\n        if not df.dtypes.apply(lambda x: x.name).to_dict() == data_types_info:\\n            raise ValueError(\"Data types do not match the expected dataset\")\\n        logger.info(df)\\n        return df\\n    except FileNotFoundError:\\n        logger.error(\"The file at the specified path was not found\")\\n        return None\\n    except pd.errors.EmptyDataError:\\n        logger.error(\"The file at the specified path is empty\")\\n        return None\\n    except pd.errors.ParserError:\\n        logger.error(\"Error parsing the file at the specified path\")\\n        return None\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 695, 'completion_tokens': 259, 'total_tokens': 954}}\u001b[0m\n",
      "\u001b[32m2024-07-18 21:37:29.052\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m192\u001b[0m - \u001b[31m\u001b[1mValidation failed for step 11: INFO:root:      age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 10, in <module>\n",
      "    validate_step()\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 7, in validate_step\n",
      "    logger.info(df)\n",
      "NameError: name 'logger' is not defined\n",
      "\u001b[0m\n",
      "\u001b[32m2024-07-18 21:37:37.123\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mopenai_chat\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mFull API response: {'id': 'gen-xOk4UhfVvFglBkVWySOYUwhUoaoi', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324250, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the fixed code:\\n\\n```\\nimport pandas as pd\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger()\\n\\ndef step_11(csv_path):\\n    try:\\n        df = pd.read_csv(csv_path)\\n        if not set(df.columns) == set([\\'age\\', \\'sex\\', \\'bmi\\', \\'children\\', \\'smoker\\', \\'region\\', \\'charges\\']):\\n            raise ValueError(\"Columns do not match the expected dataset\")\\n        data_types_info = {\\'age\\': \\'int64\\', \\'sex\\': \\'object\\', \\'bmi\\': \\'float64\\', \\'children\\': \\'int64\\', \\'smoker\\': \\'object\\', \\'region\\': \\'object\\', \\'charges\\': \\'float64\\'}\\n        if not df.dtypes.apply(lambda x: x.name).to_dict() == data_types_info:\\n            raise ValueError(\"Data types do not match the expected dataset\")\\n        logger.info(df)\\n        return df\\n    except FileNotFoundError:\\n        logger.error(\"The file at the specified path was not found\")\\n        return None\\n    except pd.errors.EmptyDataError:\\n        logger.error(\"The file at the specified path is empty\")\\n        return None\\n    except pd.errors.ParserError:\\n        logger.error(\"Error parsing the file at the specified path\")\\n        return None\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 693, 'completion_tokens': 259, 'total_tokens': 952}}\u001b[0m\n",
      "\u001b[32m2024-07-18 21:37:37.464\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m192\u001b[0m - \u001b[31m\u001b[1mValidation failed for step 11: INFO:root:      age     sex     bmi  children smoker     region      charges\n",
      "0      19  female  27.900         0    yes  southwest  16884.92400\n",
      "1      18    male  33.770         1     no  southeast   1725.55230\n",
      "2      28    male  33.000         3     no  southeast   4449.46200\n",
      "3      33    male  22.705         0     no  northwest  21984.47061\n",
      "4      32    male  28.880         0     no  northwest   3866.85520\n",
      "...   ...     ...     ...       ...    ...        ...          ...\n",
      "1333   50    male  30.970         3     no  northwest  10600.54830\n",
      "1334   18  female  31.920         0     no  northeast   2205.98080\n",
      "1335   18  female  36.850         0     no  southeast   1629.83350\n",
      "1336   21  female  25.800         0     no  southwest   2007.94500\n",
      "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
      "\n",
      "[1338 rows x 7 columns]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 10, in <module>\n",
      "    validate_step()\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_11.py\", line 7, in validate_step\n",
      "    logger.info(df)\n",
      "NameError: name 'logger' is not defined\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m selected_step_numbers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m51\u001b[39m, \u001b[38;5;241m61\u001b[39m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Generate all step files\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Generate main file\u001b[39;00m\n\u001b[1;32m     37\u001b[0m generate_main_file(workflow_steps, selected_step_numbers, csv_path)\n",
      "Cell \u001b[0;32mIn[2], line 193\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[1;32m    192\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation failed for step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 193\u001b[0m     fixed_code \u001b[38;5;241m=\u001b[39m \u001b[43mfix_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_snippet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    195\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(fixed_code)\n",
      "Cell \u001b[0;32mIn[2], line 98\u001b[0m, in \u001b[0;36mfix_code\u001b[0;34m(code_snippet, error_message, csv_path)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfix_code\u001b[39m(code_snippet, error_message, csv_path):\n\u001b[1;32m     93\u001b[0m     request \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following code snippet encountered an error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcode_snippet\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError message:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease fix the code snippet to resolve the error without providing any explanations or comments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     )\n\u001b[0;32m---> 98\u001b[0m     fixed_code \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_code_snippet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_correct_code(fixed_code, csv_path)\n",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m, in \u001b[0;36mgenerate_code_snippet\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_code_snippet\u001b[39m(request):\n\u001b[0;32m---> 73\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[2], line 58\u001b[0m, in \u001b[0;36mopenai_chat\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m     50\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m }\n\u001b[1;32m     54\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/llama-3-70b-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: request}]\n\u001b[1;32m     57\u001b[0m }\n\u001b[0;32m---> 58\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mapi_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Log the full response for debugging\u001b[39;00m\n\u001b[1;32m     61\u001b[0m response_json \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/urllib3/response.py:1040\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/urllib3/response.py:1184\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/urllib3/response.py:1108\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_main_file(workflow_steps, selected_step_numbers, csv_path):\n",
    "    main_code = \"import pandas as pd\\n\\n\"\n",
    "    for step in selected_step_numbers:\n",
    "        main_code += f\"from step_{step} import step_{step}\\n\"\n",
    "\n",
    "    main_code += \"\\ndef main():\\n\"\n",
    "    main_code += f\"    csv_path = '{csv_path}'\\n\"\n",
    "    main_code += \"    df = pd.read_csv(csv_path)\\n\"\n",
    "\n",
    "    for step in selected_step_numbers:\n",
    "        input_params = \", \".join(workflow_steps[step]['input'])\n",
    "        output_params = \", \".join(workflow_steps[step]['output'])\n",
    "        main_code += f\"    {output_params} = step_{step}({input_params})\\n\"\n",
    "\n",
    "    main_code += \"\\nif __name__ == '__main__':\\n\"\n",
    "    main_code += \"    main()\"\n",
    "\n",
    "    with open(\"main.py\", \"w\") as file:\n",
    "        file.write(main_code)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/datasets/insurance.csv\"\n",
    "    info_file_path = \"dataset_info.json\"\n",
    "\n",
    "    if not os.path.exists(info_file_path):\n",
    "        save_dataset_info(csv_path, info_file_path)\n",
    "\n",
    "    with open(info_file_path, 'r') as f:\n",
    "        dataset_info = json.load(f)\n",
    "\n",
    "    selected_step_numbers = [11, 21, 31, 32, 51, 61]\n",
    "    \n",
    "    # Generate all step files\n",
    "    main()\n",
    "    \n",
    "    # Generate main file\n",
    "    generate_main_file(workflow_steps, selected_step_numbers, csv_path)\n",
    "\n",
    "    logger.info(\"Main script generated successfully.\")\n",
    "\n",
    "    # Validate the main script\n",
    "    success, output = validate_unit_code(\"main.py\")\n",
    "    if success:\n",
    "        logger.info(\"Main script validated successfully.\")\n",
    "    else:\n",
    "        logger.error(f\"Validation failed for main script.\")\n",
    "        logger.error(f\"Error: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Tuple, Any, Set\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the workflow steps with assigned numbers\n",
    "workflow_steps = {\n",
    "    11: {\n",
    "        \"description\": \"Load the CSV file into a suitable format (e.g., DataFrame)\",\n",
    "        \"dependencies\": []\n",
    "    },\n",
    "    21: {\n",
    "        \"description\": \"Examine the structure and characteristics of the data\",\n",
    "        \"dependencies\": [11]\n",
    "    },\n",
    "    22: {\n",
    "        \"description\": \"Identify missing values, data types, and statistical summary\",\n",
    "        \"dependencies\": [11, 21]\n",
    "    },\n",
    "    23: {\n",
    "        \"description\": \"Visualize the data using charts, graphs, or plots\",\n",
    "        \"dependencies\": [11, 21, 22]\n",
    "    },\n",
    "    24: {\n",
    "        \"description\": \"Gain insights and formulate hypotheses\",\n",
    "        \"dependencies\": [11, 21, 22, 23]\n",
    "    },\n",
    "    31: {\n",
    "        \"description\": \"Handle missing values (remove or impute)\",\n",
    "        \"dependencies\": [11, 22]\n",
    "    },\n",
    "    32: {\n",
    "        \"description\": \"Convert categorical variables to numerical representations\",\n",
    "        \"dependencies\": [11, 22, 31]\n",
    "    },\n",
    "    33: {\n",
    "        \"description\": \"Perform feature scaling or normalization\",\n",
    "        \"dependencies\": [11, 31, 32]\n",
    "    },\n",
    "    34: {\n",
    "        \"description\": \"Encode categorical variables (one-hot encoding, label encoding, etc.)\",\n",
    "        \"dependencies\": [11, 32]\n",
    "    },\n",
    "    35: {\n",
    "        \"description\": \"Split the preprocessed data into training and testing sets. Ensure that all previous data preprocessing steps (handling missing values, encoding categorical variables, and scaling) have been completed before this step.\",\n",
    "        \"dependencies\": [11, 31, 32, 33, 34]\n",
    "    },\n",
    "    41: {\n",
    "        \"description\": \"Create new features based on domain knowledge or data insights\",\n",
    "        \"dependencies\": [11, 24, 35]\n",
    "    },\n",
    "    42: {\n",
    "        \"description\": \"Combine or transform existing features\",\n",
    "        \"dependencies\": [11, 35, 41]\n",
    "    },\n",
    "    43: {\n",
    "        \"description\": \"Perform feature selection to identify relevant features\",\n",
    "        \"dependencies\": [11, 35, 41, 42]\n",
    "    },\n",
    "    51: {\n",
    "        \"description\": \"Analyze the problem type and dataset characteristics, then select and implement the single most appropriate machine learning algorithm (choose from scikit-learn, XGBoost, LightGBM, or CatBoost). Justify your choice based on the data properties and problem requirements.\",\n",
    "        \"dependencies\": [35, 43]\n",
    "    },\n",
    "    52: {\n",
    "        \"description\": \"Define the model architecture and hyperparameters\",\n",
    "        \"dependencies\": [51]\n",
    "    },\n",
    "    53: {\n",
    "        \"description\": \"Train the selected model on the training data\",\n",
    "        \"dependencies\": [35, 51, 52]\n",
    "    },\n",
    "    54: {\n",
    "        \"description\": \"Utilize techniques like cross-validation for model evaluation\",\n",
    "        \"dependencies\": [35, 51, 52, 53]\n",
    "    },\n",
    "    61: {\n",
    "        \"description\": \"Evaluate the trained model's performance on the testing data\",\n",
    "        \"dependencies\": [35, 53, 54]\n",
    "    },\n",
    "    62: {\n",
    "        \"description\": \"Calculate evaluation metrics (e.g., accuracy, precision, recall, F1-score)\",\n",
    "        \"dependencies\": [61]\n",
    "    },\n",
    "    63: {\n",
    "        \"description\": \"Visualize the model's performance using confusion matrix, ROC curve, etc.\",\n",
    "        \"dependencies\": [61, 62]\n",
    "    },\n",
    "    64: {\n",
    "        \"description\": \"Fine-tune the model if necessary\",\n",
    "        \"dependencies\": [61, 62, 63]\n",
    "    },\n",
    "    71: {\n",
    "        \"description\": \"Analyze the model's coefficients or feature importances\",\n",
    "        \"dependencies\": [53, 61]\n",
    "    },\n",
    "    72: {\n",
    "        \"description\": \"Visualize the model's decision boundaries or learned patterns\",\n",
    "        \"dependencies\": [53, 61, 71]\n",
    "    },\n",
    "    73: {\n",
    "        \"description\": \"Interpret the model's predictions and explain its behavior\",\n",
    "        \"dependencies\": [53, 61, 71, 72]\n",
    "    },\n",
    "    81: {\n",
    "        \"description\": \"Generate unit code documentation during the code generation process\",\n",
    "        \"dependencies\": [11, 21, 22, 23, 24, 31, 32, 33, 34, 35, 41, 42, 43, 51, 52, 53, 54, 61, 62, 63, 64, 71, 72, 73]\n",
    "    },\n",
    "    82: {\n",
    "        \"description\": \"Execute the combined code and capture relevant outputs and insights\",\n",
    "        \"dependencies\": [11, 21, 22, 23, 24, 31, 32, 33, 34, 35, 41, 42, 43, 51, 52, 53, 54, 61, 62, 63, 64, 71, 72, 73]\n",
    "    },\n",
    "    83: {\n",
    "        \"description\": \"Create a comprehensive documentation for the entire workflow, including project overview, dataset details, selected steps, results, and interpretations\",\n",
    "        \"dependencies\": [81, 82]\n",
    "    },\n",
    "    84: {\n",
    "        \"description\": \"Present the documentation to users for understanding and reference\",\n",
    "        \"dependencies\": [83]\n",
    "    }\n",
    "}\n",
    "\n",
    "# OpenAI API configuration\n",
    "api_url = \"https://openrouter.ai/api/v1\"\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "\n",
    "def openai_chat(request):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"deepseek/deepseek-chat\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": request}]\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/chat/completions\", headers=headers, json=data)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_code_for_step() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 162\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocumentation saved to documentation.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 122\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m selected_step_numbers:\n\u001b[0;32m--> 122\u001b[0m     full_code, df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_code_for_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkflow_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     code_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_code.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(code_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_code_for_step() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "def generate_code_snippet(request: str) -> str:\n",
    "    response = openai_chat(request)\n",
    "    return response\n",
    "\n",
    "def clean_and_correct_code(generated_code: str, csv_path: str) -> str:\n",
    "    cleaned_code = generated_code.replace(\"```\", \"\").strip()\n",
    "    cleaned_code_lines = cleaned_code.split(\"\\n\")\n",
    "    cleaned_code_lines = [\n",
    "        line for line in cleaned_code_lines if not line.lower().startswith(\"here is the\")]\n",
    "    cleaned_code = \"\\n\".join(cleaned_code_lines)\n",
    "    if \"python\" in cleaned_code:\n",
    "        cleaned_code = cleaned_code.split(\"python\")[1].strip()\n",
    "    corrected_code = cleaned_code.replace(\"{csv_path}\", f\"{csv_path}\")\n",
    "    return corrected_code\n",
    "\n",
    "def get_dataset_info(csv_path: str) -> Tuple[List[str], Dict[str, Any], Dict[str, List[Any]], Dict[str, Dict[Any, int]], Dict[str, Dict[str, float]]]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    columns = df.columns.tolist()\n",
    "    types = df.dtypes.to_dict()\n",
    "    sample_data = df.head().to_dict(orient='list')\n",
    "    value_counts = {col: df[col].value_counts().to_dict()\n",
    "                    for col in df.columns}\n",
    "    description = df.describe().to_dict()\n",
    "    return columns, types, sample_data, value_counts, description\n",
    "\n",
    "def validate_unit_code(code_filename: str) -> Tuple[bool, str]:\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", code_filename], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(result.stderr)\n",
    "        return True, result.stdout\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def generate_documentation(step: int, columns_info: str, types_info: str, sample_data_info: str, value_counts_info: str, description_info: str) -> str:\n",
    "    request = (\n",
    "        f\"Provide a clear and concise description of the job performed by the code for the following step: {workflow_steps[step]['description']}. \"\n",
    "        f\"The description should summarize the main tasks and key points without going into the specifics of the code. \"\n",
    "        f\"The dataset has the following columns: {columns_info}. \"\n",
    "        f\"The data types are: {types_info}. Sample data: {sample_data_info}. Value counts: {value_counts_info}. \"\n",
    "        f\"Description: {description_info}.\"\n",
    "    )\n",
    "    documentation = generate_code_snippet(request)\n",
    "    return documentation\n",
    "\n",
    "def fix_code(code_snippet: str, error_message: str, csv_path: str) -> str:\n",
    "    request = (\n",
    "        f\"The following code snippet encountered an error:\\n\\n{code_snippet}\\n\\n\"\n",
    "        f\"Error message:\\n{error_message}\\n\\n\"\n",
    "        f\"Please fix the code snippet to resolve the error without providing any explanations or comments.\"\n",
    "    )\n",
    "    fixed_code = generate_code_snippet(request)\n",
    "    cleaned_fixed_code = clean_and_correct_code(fixed_code, csv_path)\n",
    "    return cleaned_fixed_code\n",
    "\n",
    "def get_all_prerequisites(step: int, workflow_steps: Dict[int, Dict[str, Any]]) -> Set[int]:\n",
    "    prerequisites = set()\n",
    "    for prereq in workflow_steps[step]['dependencies']:\n",
    "        prerequisites.add(prereq)\n",
    "        prerequisites.update(get_all_prerequisites(prereq, workflow_steps))\n",
    "    return prerequisites\n",
    "\n",
    "def generate_code_for_step(step: int, workflow_steps: Dict[int, Dict[str, Any]], generated_code: Dict[int, str], csv_path: str) -> str:\n",
    "    prerequisites = get_all_prerequisites(step, workflow_steps)\n",
    "    full_code = \"\"\n",
    "    \n",
    "    # Add import statements only once at the beginning\n",
    "    import_statements = set()\n",
    "    import_statements.add(f\"import pandas as pd\\n\\n\")\n",
    "    import_statements.add(f\"# Load the dataset\\ndf = pd.read_csv('{csv_path}')\\n\\n\")\n",
    "    \n",
    "    # Add code from prerequisites without repeating imports\n",
    "    for prereq in sorted(prerequisites):\n",
    "        if prereq not in generated_code:\n",
    "            generated_code[prereq] = generate_code_snippet_for_step(prereq, workflow_steps, csv_path)\n",
    "        \n",
    "        prereq_code = generated_code[prereq]\n",
    "        # Remove import statements and df loading from prerequisite code\n",
    "        prereq_code = '\\n'.join([line for line in prereq_code.split('\\n') if not line.startswith('import') and 'pd.read_csv' not in line])\n",
    "        \n",
    "        full_code += f\"# Code from step {prereq}: {workflow_steps[prereq]['description']}\\n{prereq_code}\\n\\n\"\n",
    "    \n",
    "    current_step_code = generate_code_snippet_for_step(step, workflow_steps, csv_path)\n",
    "    # Remove import statements and df loading from current step code\n",
    "    current_step_code = '\\n'.join([line for line in current_step_code.split('\\n') if not line.startswith('import') and 'pd.read_csv' not in line])\n",
    "    \n",
    "    full_code += f\"# Code for current step {step}: {workflow_steps[step]['description']}\\n{current_step_code}\"\n",
    "    \n",
    "    # Combine import statements with the rest of the code\n",
    "    full_code = ''.join(import_statements) + full_code\n",
    "    \n",
    "    return full_code\n",
    "\n",
    "def generate_code_snippet_for_step(step: int, workflow_steps: Dict[int, Dict[str, Any]], csv_path: str) -> str:\n",
    "    request = (\n",
    "        f\"Write a Python code snippet for the following step: {workflow_steps[step]['description']}. \"\n",
    "        f\"Assume that the dataset has already been loaded into a DataFrame named 'df'. \"\n",
    "        f\"Do not include import statements or code to load the dataset. \"\n",
    "        f\"Only return the code specific to this step without any additional explanations.\"\n",
    "    )\n",
    "    code_snippet = generate_code_snippet(request)\n",
    "    cleaned_code_snippet = clean_and_correct_code(code_snippet, csv_path)\n",
    "    return cleaned_code_snippet\n",
    "\n",
    "def main():\n",
    "    csv_path = \"/Users/ilya/Desktop/GitHub_Repositories/HW_University/Data_Mining/datasets/insurance.csv\"\n",
    "    columns, types, sample_data, value_counts, description = get_dataset_info(csv_path)\n",
    "\n",
    "    columns_info = \", \".join(columns)\n",
    "    types_info = \", \".join([f\"{col}: {typ}\" for col, typ in types.items()])\n",
    "    sample_data_info = \", \".join([f\"{col}: {vals[:5]}\" for col, vals in sample_data.items()])\n",
    "    value_counts_info = \", \".join([f\"{col}: {dict(list(vc.items())[:5])}\" for col, vc in value_counts.items()])\n",
    "    description_info = \", \".join([f\"{col}: {desc}\" for col, desc in description.items()])\n",
    "\n",
    "    selected_step_numbers = [11, 21, 22, 31, 32, 35, 51, 52, 53, 61, 62]\n",
    "    generated_code = {}\n",
    "    documentation_snippets = []\n",
    "    df = None\n",
    "\n",
    "    for step in selected_step_numbers:\n",
    "        full_code, df = generate_code_for_step(step, workflow_steps, generated_code, csv_path, df)\n",
    "        \n",
    "        code_filename = f\"step_{step}_code.py\"\n",
    "        with open(code_filename, \"w\") as file:\n",
    "            file.write(full_code)\n",
    "        \n",
    "        success, output = validate_unit_code(code_filename)\n",
    "        while not success:\n",
    "            print(f\"Validation failed for step {step}: {output}\")\n",
    "            fixed_code, df = fix_code(full_code, output, csv_path, df)\n",
    "            with open(code_filename, \"w\") as file:\n",
    "                file.write(fixed_code)\n",
    "            success, output = validate_unit_code(code_filename)\n",
    "        \n",
    "        generated_code[step] = full_code\n",
    "        documentation_snippet = generate_documentation(\n",
    "            step, columns_info, types_info, sample_data_info, value_counts_info, description_info)\n",
    "        documentation_snippets.append(documentation_snippet)\n",
    "\n",
    "    # Combine all code snippets\n",
    "    combined_code = \"\\n\\n\".join([generated_code[step] for step in selected_step_numbers])\n",
    "\n",
    "    # Save the combined code to a file\n",
    "    with open(\"combined_code.py\", \"w\") as file:\n",
    "        file.write(combined_code)\n",
    "\n",
    "    # Validate the combined code\n",
    "    success, output = validate_unit_code(\"combined_code.py\")\n",
    "    if success:\n",
    "        print(\"Combined code validated successfully.\")\n",
    "    else:\n",
    "        print(f\"Validation failed for combined code.\")\n",
    "        print(f\"Error: {output}\")\n",
    "\n",
    "    # Save documentation to a separate file\n",
    "    with open(\"documentation.txt\", \"w\") as file:\n",
    "        file.write(\"\\n\\n\".join(documentation_snippets))\n",
    "    print(\"Documentation saved to documentation.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "from loguru import logger\n",
    "\n",
    "from graph import Step\n",
    "from language_modeling import OpenAiLlamaApi, LlamaModel, PromptGenerator\n",
    "from code_generation import ValidationCodeGenerator, MainCodeGenerator\n",
    "from orchestrator import Orchestrator\n",
    "from utils import get_dataset_info\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logger\n",
    "logger.add(\"execution.log\", rotation=\"500 MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_STEP_SCRIPT = \"\"\"\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def step_40(Segments_normalized, Dec_levels):\n",
    "    Features = []\n",
    "    for segment in Segments_normalized:\n",
    "        coeffs = pywt.wavedec(segment, 'db4', level=Dec_levels)\n",
    "        features = [coefficient.mean() for coefficient in coeffs]\n",
    "        Features.append(features)\n",
    "    return StandardScaler().fit_transform(Features)\n",
    "\"\"\"\n",
    "\n",
    "MODEL_TAG = \"meta-llama/llama-3-70b-instruct\"\n",
    "\n",
    "SIMPLE_CASE_CSV_PATH = \"/Users/ilya/Desktop/CodeGeneration-main/datasets/insurance.csv\"\n",
    "COMPLICATED_CASE_CSV_PATH = \"/Users/ilya/Desktop/CodeGeneration-main/datasets/learning-file_2.csv\"\n",
    "\n",
    "PYTHON_PATH = \"/Users/ilya/thesis_env/myenv/bin/python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_simple_case = [\n",
    "    Step(\n",
    "        step_id=\"11\",\n",
    "        description=\"Load the CSV file as pandas DataFrame\",\n",
    "        dependencies=[],\n",
    "        input_vars=[\"csv_path\"],\n",
    "        output_vars=[\"df\"],\n",
    "        additional_info=\"\"\n",
    "    ),\n",
    "    Step(\n",
    "        step_id=\"21\",\n",
    "        description=\"Examine the structure and characteristics of the data\",\n",
    "        dependencies=[\"11\"],\n",
    "        input_vars=[\"df\"],\n",
    "        output_vars=[\"structure_info\"],\n",
    "        additional_info=\"\"\n",
    "    ),\n",
    "    Step(\n",
    "        step_id=\"31\",\n",
    "        description=\"Identify missing values, data types, and handle missing values if there are any\",\n",
    "        dependencies=[\"11\", \"21\"],\n",
    "        input_vars=[\"df\"],\n",
    "        output_vars=[\"df_cleaned\", \"data_types_info\"],\n",
    "        additional_info=\"\"\n",
    "    ),\n",
    "    Step(\n",
    "        step_id=\"41\",\n",
    "        description=\"Identify if there is a need to convert categorical variables to numerical representations. If yes, then convert them.\",\n",
    "        dependencies=[\"11\", \"31\"],\n",
    "        input_vars=[\"df_cleaned\", \"data_types_info\"],\n",
    "        output_vars=[\"df_encoded\"],\n",
    "        additional_info=\"\"\n",
    "    ),\n",
    "    Step(\n",
    "        step_id=\"51\",\n",
    "        description=\"Split the preprocessed data into training and testing sets, and implement a machine learning algorithm (choose from scikit-learn, XGBoost, LightGBM, or CatBoost).\",\n",
    "        dependencies=[\"11\", \"31\", \"41\"],\n",
    "        input_vars=[\"df_encoded\"],\n",
    "        output_vars=[\"model\", \"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n",
    "        additional_info=\"\"\n",
    "    ),\n",
    "    Step(step_id=\"61\",\n",
    "        description=\"Evaluate the model's performance on both training and testing data, calculate evaluation metrics (for classification: [accuracy, precision, recall, F1-score]; for regression: [R^2, MSE, RMSE]), and compare the difference.\",\n",
    "        dependencies=[\"51\"],\n",
    "        input_vars=[\"model\", \"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n",
    "        output_vars=[\"evaluation_results\", \"metrics\"],\n",
    "        additional_info=\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_complicated_case = [\n",
    "    Step(\n",
    "        step_id=\"10\",\n",
    "        description=\"Import raw data from CSV and segment it\",\n",
    "        dependencies=[],\n",
    "        input_vars=[\"csv_path\", \"SizeSegment\"],\n",
    "        output_vars=[\"Segments\"],\n",
    "        additional_info=\"Use pandas to read the CSV and create segments of size SizeSegment.\"\n",
    "    ),\n",
    "    Step(\n",
    "        step_id=\"20\",\n",
    "        description=\"Normalize the segmented data using MinMaxScaler\",\n",
    "        dependencies=[\"10\"],\n",
    "        input_vars=[\"Segments\"],\n",
    "        output_vars=[\"Segments_normalized\"],\n",
    "        additional_info=\"Segments is a list of 1D numpy arrays. Each segment should be normalized independently.\"\n",
    "    ),\n",
    "    Step(\n",
    "        step_id=\"30\",\n",
    "        description=\"Extract features using wavelet decomposition\",\n",
    "        dependencies=[\"20\"],\n",
    "        input_vars=[\"Segments_normalized\", \"Dec_levels\"],\n",
    "        output_vars=[\"Features\"],\n",
    "        additional_info=\"Use pywavelets (pywt) library with 'db3' wavelet and specified Dec_levels.\"\n",
    "    ),\n",
    "    Step(\n",
    "        step_id=\"40\",\n",
    "        description=\"Apply PCA for dimension reduction\",\n",
    "        dependencies=[\"30\"],\n",
    "        input_vars=[\"Features\", \"NC_pca\"],\n",
    "        output_vars=[\"PCA_Features\", \"pca\"],\n",
    "        additional_info=\"Use sklearn's PCA. Return both the transformed features and the PCA object.\"\n",
    "    ),\n",
    "    Step(step_id=\"50\",\n",
    "        description=\"Train model, evaluate, and calculate metrics\",\n",
    "        dependencies=[\"40\"],\n",
    "        input_vars=[\"PCA_Features\", \"kernel\", \"nu\", \"gamma\"],\n",
    "        output_vars=[\"FittedClassifier\", \"Prec_learn\", \"Prec_test\"],\n",
    "        additional_info=\"\"\"\n",
    "        1. Create labels: np.ones for learning data.\n",
    "        2. Split data into train and test sets (80% train, 20% test).\n",
    "        3. Create and fit a One-Class SVM classifier using sklearn.\n",
    "        4. Predict labels for training data.\n",
    "        5. Calculate error rate for training data.\n",
    "        6. Predict labels for test data (assume all test data as anomaly, i.e., -1).\n",
    "        7. Calculate error rate for test data.\n",
    "        8. Calculate precision as 1 - error_rate for both training and test.\n",
    "        Return the fitted classifier and both precision values.\n",
    "        \"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('env.json', 'r') as f:\n",
    "    credentials_dict = json.load(f)\n",
    "\n",
    "API_URL = \"https://openrouter.ai/api/v1\"\n",
    "API_KEY = credentials_dict[\"OPENROUTER_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_api = OpenAiLlamaApi(API_URL, API_KEY, MODEL_TAG)\n",
    "model = LlamaModel(llama_api)\n",
    "validation_code_genrator = ValidationCodeGenerator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = SIMPLE_CASE_CSV_PATH\n",
    "raw_data = pd.read_csv(csv_path)\n",
    "dataset_info_simple_case = get_dataset_info(raw_data)\n",
    "\n",
    "\n",
    "parameters_simple_case = {\n",
    "    'csv_path': f\"'{csv_path}'\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_code_generator = MainCodeGenerator(additional_lines=[])\n",
    "prompt_generator = PromptGenerator(EXAMPLE_STEP_SCRIPT, dataset_info_simple_case)\n",
    "orchestrator = Orchestrator(\n",
    "    model,\n",
    "    prompt_generator,\n",
    "    validation_code_genrator,\n",
    "    main_code_generator,\n",
    "    'out_simple_case',\n",
    "    PYTHON_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-14 22:59:14.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mFull API response: {'id': 'gen-PQRpqxdpNL3LoQb84rKl7cozxlyp', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723661952, 'choices': [{'logprobs': None, 'finish_reason': 'eos', 'index': 0, 'message': {'role': 'assistant', 'content': 'Here is the Python function named \\'step_11\\' to load the CSV file as a pandas DataFrame:\\n```\\nimport pandas as pd\\n\\ndef step_11(csv_path):\\n    try:\\n        df = pd.read_csv(csv_path)\\n        return df\\n    except FileNotFoundError:\\n        print(\"File not found. Please check the file path.\")\\n        return None\\n```'}}], 'usage': {'prompt_tokens': 1068, 'completion_tokens': 74, 'total_tokens': 1142}}\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:14.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_step_file\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mgenerated step source for step 11, filename: step_11.py,\n",
      "source:\n",
      " \n",
      "import pandas as pd\n",
      "\n",
      "def step_11(csv_path):\n",
      "    try:\n",
      "        df = pd.read_csv(csv_path)\n",
      "        return df\n",
      "    except FileNotFoundError:\n",
      "        print(\"File not found. Please check the file path.\")\n",
      "        return None\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:14.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_validation_file\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mgenerated validation source for step 11, filename: validate_step_11.py,\n",
      "source:\n",
      " import pandas as pd\n",
      "from step_11 import step_11\n",
      "\n",
      "\n",
      "csv_path = '/Users/ilya/Desktop/CodeGeneration-main/datasets/insurance.csv'\n",
      "\n",
      "def validate_step():\n",
      "    df = step_11(csv_path)\n",
      "    print(df)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:14.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mrunning validate_step_11.py in /Users/ilya/Desktop/CodeGeneration-main/src/out_simple_case\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:14.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1m/Users/ilya/thesis_env/myenv/bin/python validate_step_11.py\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:14.406\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m240\u001b[0m - \u001b[34m\u001b[1mexit_code = 0\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:22.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mFull API response: {'id': 'gen-QVpdE0US402zW51pv1Ul6GiIFK2L', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723661955, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the Python function named 'step_21':\\n\\n```\\nimport pandas as pd\\n\\ndef step_21(df):\\n    structure_info = {}\\n    structure_info['data_types'] = df.dtypes.to_dict()\\n    structure_info['value_counts'] = {col: df[col].value_counts().head(5).to_dict() for col in df.columns}\\n    structure_info['statistical_description'] = df.describe().to_dict()\\n    return structure_info\\n```\"}}], 'usage': {'prompt_tokens': 1069, 'completion_tokens': 94, 'total_tokens': 1163}}\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:22.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_step_file\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mgenerated step source for step 21, filename: step_21.py,\n",
      "source:\n",
      " \n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "def step_21(df):\n",
      "    structure_info = {}\n",
      "    structure_info['data_types'] = df.dtypes.to_dict()\n",
      "    structure_info['value_counts'] = {col: df[col].value_counts().head(5).to_dict() for col in df.columns}\n",
      "    structure_info['statistical_description'] = df.describe().to_dict()\n",
      "    return structure_info\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:22.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_validation_file\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mgenerated validation source for step 21, filename: validate_step_21.py,\n",
      "source:\n",
      " import pandas as pd\n",
      "from step_11 import step_11\n",
      "from step_21 import step_21\n",
      "\n",
      "\n",
      "csv_path = '/Users/ilya/Desktop/CodeGeneration-main/datasets/insurance.csv'\n",
      "\n",
      "def validate_step():\n",
      "    df = step_11(csv_path)\n",
      "    structure_info = step_21(df)\n",
      "    print(structure_info)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:22.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mrunning validate_step_21.py in /Users/ilya/Desktop/CodeGeneration-main/src/out_simple_case\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:22.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1m/Users/ilya/thesis_env/myenv/bin/python validate_step_21.py\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:22.471\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m240\u001b[0m - \u001b[34m\u001b[1mexit_code = 0\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:27.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mFull API response: {'id': 'gen-zLkxwCKSNOcNiEEqU1coDN2kiyDL', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723661963, 'choices': [{'logprobs': None, 'finish_reason': 'eos', 'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the Python function `step_31` that meets the requirements:\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\ndef step_31(df):\\n    # Identify missing values\\n    missing_values = df.isnull().sum()\\n    \\n    # Handle missing values if there are any\\n    if missing_values.any():\\n        df_cleaned = df.apply(lambda x: x.fillna(x.mean() if x.dtype == 'float64' else x.mode()[0]))\\n    else:\\n        df_cleaned = df\\n    \\n    # Get data types info\\n    data_types_info = df_cleaned.dtypes\\n    \\n    return df_cleaned, data_types_info\\n```\\nNote that I used the `.isnull().sum()` method to identify missing values, and then used the `.fillna()` method to fill them with the mean or mode of the respective column, depending on its data type. I also used the `.dtypes` attribute to get the data types info.\"}}], 'usage': {'prompt_tokens': 1081, 'completion_tokens': 197, 'total_tokens': 1278}}\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:27.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_step_file\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mgenerated step source for step 31, filename: step_31.py,\n",
      "source:\n",
      " \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "def step_31(df):\n",
      "    # Identify missing values\n",
      "    missing_values = df.isnull().sum()\n",
      "    \n",
      "    # Handle missing values if there are any\n",
      "    if missing_values.any():\n",
      "        df_cleaned = df.apply(lambda x: x.fillna(x.mean() if x.dtype == 'float64' else x.mode()[0]))\n",
      "    else:\n",
      "        df_cleaned = df\n",
      "    \n",
      "    # Get data types info\n",
      "    data_types_info = df_cleaned.dtypes\n",
      "    \n",
      "    return df_cleaned, data_types_info\n",
      "\n",
      "Note that I used the `.isnull().sum()` method to identify missing values, and then used the `.fillna()` method to fill them with the mean or mode of the respective column, depending on its data type. I also used the `.dtypes` attribute to get the data types info.\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:27.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_validation_file\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mgenerated validation source for step 31, filename: validate_step_31.py,\n",
      "source:\n",
      " import pandas as pd\n",
      "from step_11 import step_11\n",
      "from step_21 import step_21\n",
      "from step_31 import step_31\n",
      "\n",
      "\n",
      "csv_path = '/Users/ilya/Desktop/CodeGeneration-main/datasets/insurance.csv'\n",
      "\n",
      "def validate_step():\n",
      "    df = step_11(csv_path)\n",
      "    structure_info = step_21(df)\n",
      "    df_cleaned, data_types_info = step_31(df)\n",
      "    print(df_cleaned, data_types_info)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:27.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mrunning validate_step_31.py in /Users/ilya/Desktop/CodeGeneration-main/src/out_simple_case\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:27.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1m/Users/ilya/thesis_env/myenv/bin/python validate_step_31.py\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:27.869\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m240\u001b[0m - \u001b[34m\u001b[1mexit_code = 1\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:31.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mFull API response: {'id': 'gen-FeGQoTMf5rHEgxnnCde8gS8yptfh', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723661968, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the corrected code snippet:\\n\\n\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\ndef step_31(df):\\n    missing_values = df.isnull().sum()\\n    \\n    if missing_values.any():\\n        df_cleaned = df.apply(lambda x: x.fillna(x.mean() if x.dtype == 'float64' else x.mode()[0]) if x.hasnans else x)\\n    else:\\n        df_cleaned = df\\n    \\n    data_types_info = df_cleaned.dtypes\\n    \\n    return df_cleaned, data_types_info\\n```\"}}], 'system_fingerprint': '601a0519fb4d41a706042f153a1732dce93cd158a93180a364be77ef4864bd39', 'usage': {'prompt_tokens': 392, 'completion_tokens': 112, 'total_tokens': 504}}\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:31.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mfix_step_source\u001b[0m:\u001b[36m216\u001b[0m - \u001b[1mfixing step source for step 31, filename: step_31.py,\n",
      "source: \n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "def step_31(df):\n",
      "    missing_values = df.isnull().sum()\n",
      "    \n",
      "    if missing_values.any():\n",
      "        df_cleaned = df.apply(lambda x: x.fillna(x.mean() if x.dtype == 'float64' else x.mode()[0]) if x.hasnans else x)\n",
      "    else:\n",
      "        df_cleaned = df\n",
      "    \n",
      "    data_types_info = df_cleaned.dtypes\n",
      "    \n",
      "    return df_cleaned, data_types_info\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:31.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mrunning validate_step_31.py in /Users/ilya/Desktop/CodeGeneration-main/src/out_simple_case\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:31.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1m/Users/ilya/thesis_env/myenv/bin/python validate_step_31.py\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:31.425\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m240\u001b[0m - \u001b[34m\u001b[1mexit_code = 0\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:36.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mFull API response: {'id': 'gen-YYzhsZjPLvrh8TOeHPXoW1lwvo0a', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723661972, 'choices': [{'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}, 'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the Python function named 'step_41':\\n\\n```\\nimport pandas as pd\\nfrom sklearn.preprocessing import LabelEncoder\\n\\ndef step_41(df_cleaned, data_types_info):\\n    df_encoded = df_cleaned.copy()\\n    categorical_columns = [col for col, dtype in data_types_info.items() if dtype == 'object']\\n    le = LabelEncoder()\\n    for col in categorical_columns:\\n        df_encoded[col] = le.fit_transform(df_encoded[col])\\n    return df_encoded\\n```\"}}], 'usage': {'prompt_tokens': 1086, 'completion_tokens': 101, 'total_tokens': 1187}}\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:36.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_step_file\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mgenerated step source for step 41, filename: step_41.py,\n",
      "source:\n",
      " \n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "def step_41(df_cleaned, data_types_info):\n",
      "    df_encoded = df_cleaned.copy()\n",
      "    categorical_columns = [col for col, dtype in data_types_info.items() if dtype == 'object']\n",
      "    le = LabelEncoder()\n",
      "    for col in categorical_columns:\n",
      "        df_encoded[col] = le.fit_transform(df_encoded[col])\n",
      "    return df_encoded\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:36.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_validation_file\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mgenerated validation source for step 41, filename: validate_step_41.py,\n",
      "source:\n",
      " import pandas as pd\n",
      "from step_11 import step_11\n",
      "from step_21 import step_21\n",
      "from step_31 import step_31\n",
      "from step_41 import step_41\n",
      "\n",
      "\n",
      "csv_path = '/Users/ilya/Desktop/CodeGeneration-main/datasets/insurance.csv'\n",
      "\n",
      "def validate_step():\n",
      "    df = step_11(csv_path)\n",
      "    structure_info = step_21(df)\n",
      "    df_cleaned, data_types_info = step_31(df)\n",
      "    df_encoded = step_41(df_cleaned, data_types_info)\n",
      "    print(df_encoded)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:36.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mrunning validate_step_41.py in /Users/ilya/Desktop/CodeGeneration-main/src/out_simple_case\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:36.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1m/Users/ilya/thesis_env/myenv/bin/python validate_step_41.py\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:36.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m240\u001b[0m - \u001b[34m\u001b[1mexit_code = 0\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:42.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mFull API response: {'id': 'gen-Mxy59aGE85cWSoNr9SKSVInbzpBX', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723661978, 'choices': [{'logprobs': None, 'finish_reason': 'stop', 'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the Python function `step_51` that meets the requirements:\\n```\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.preprocessing import LabelEncoder\\n\\ndef step_51(df_encoded):\\n    # Encode categorical variables\\n    le = LabelEncoder()\\n    df_encoded['sex'] = le.fit_transform(df_encoded['sex'])\\n    df_encoded['smoker'] = le.fit_transform(df_encoded['smoker'])\\n    df_encoded['region'] = le.fit_transform(df_encoded['region'])\\n\\n    # Split data into features and target\\n    X = df_encoded.drop('charges', axis=1)\\n    y = df_encoded['charges']\\n\\n    # Split data into training and testing sets\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    # Initialize and train a random forest classifier\\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\\n    model.fit(X_train, y_train)\\n\\n    return model, X_train, X_test, y_train, y_test\\n```\"}}], 'system_fingerprint': '601a0519fb4d41a706042f153a1732dce93cd158a93180a364be77ef4864bd39', 'usage': {'prompt_tokens': 1142, 'completion_tokens': 235, 'total_tokens': 1377}}\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:42.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_step_file\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mgenerated step source for step 51, filename: step_51.py,\n",
      "source:\n",
      " \n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "def step_51(df_encoded):\n",
      "    # Encode categorical variables\n",
      "    le = LabelEncoder()\n",
      "    df_encoded['sex'] = le.fit_transform(df_encoded['sex'])\n",
      "    df_encoded['smoker'] = le.fit_transform(df_encoded['smoker'])\n",
      "    df_encoded['region'] = le.fit_transform(df_encoded['region'])\n",
      "\n",
      "    # Split data into features and target\n",
      "    X = df_encoded.drop('charges', axis=1)\n",
      "    y = df_encoded['charges']\n",
      "\n",
      "    # Split data into training and testing sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "    # Initialize and train a random forest classifier\n",
      "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "    model.fit(X_train, y_train)\n",
      "\n",
      "    return model, X_train, X_test, y_train, y_test\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:42.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_validation_file\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mgenerated validation source for step 51, filename: validate_step_51.py,\n",
      "source:\n",
      " import pandas as pd\n",
      "from step_11 import step_11\n",
      "from step_21 import step_21\n",
      "from step_31 import step_31\n",
      "from step_41 import step_41\n",
      "from step_51 import step_51\n",
      "\n",
      "\n",
      "csv_path = '/Users/ilya/Desktop/CodeGeneration-main/datasets/insurance.csv'\n",
      "\n",
      "def validate_step():\n",
      "    df = step_11(csv_path)\n",
      "    structure_info = step_21(df)\n",
      "    df_cleaned, data_types_info = step_31(df)\n",
      "    df_encoded = step_41(df_cleaned, data_types_info)\n",
      "    model, X_train, X_test, y_train, y_test = step_51(df_encoded)\n",
      "    print(model, X_train, X_test, y_train, y_test)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:42.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mrunning validate_step_51.py in /Users/ilya/Desktop/CodeGeneration-main/src/out_simple_case\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:42.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1m/Users/ilya/thesis_env/myenv/bin/python validate_step_51.py\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:43.133\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m240\u001b[0m - \u001b[34m\u001b[1mexit_code = 1\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:53.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mFull API response: {'id': 'gen-8OMwpoHzHPHQiV26xminQYyjcQbr', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723661984, 'choices': [{'logprobs': None, 'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the corrected code snippet:\\n\\n```\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import LabelEncoder\\n\\ndef step_51(df_encoded):\\n    # Encode categorical variables\\n    le = LabelEncoder()\\n    df_encoded['sex'] = le.fit_transform(df_encoded['sex'])\\n    df_encoded['smoker'] = le.fit_transform(df_encoded['smoker'])\\n    df_encoded['region'] = le.fit_transform(df_encoded['region'])\\n\\n    # Split data into features and target\\n    X = df_encoded.drop('charges', axis=1)\\n    y = df_encoded['charges']\\n\\n    # Split data into training and testing sets\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    # Initialize and train a random forest regressor\\n    model = RandomForestRegressor(n_estimators=100, random_state=42)\\n    model.fit(X_train, y_train)\\n\\n    return model, X_train, X_test, y_train, y_test\\n```\"}}], 'usage': {'prompt_tokens': 634, 'completion_tokens': 229, 'total_tokens': 863}}\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:53.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mfix_step_source\u001b[0m:\u001b[36m216\u001b[0m - \u001b[1mfixing step source for step 51, filename: step_51.py,\n",
      "source: \n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "def step_51(df_encoded):\n",
      "    # Encode categorical variables\n",
      "    le = LabelEncoder()\n",
      "    df_encoded['sex'] = le.fit_transform(df_encoded['sex'])\n",
      "    df_encoded['smoker'] = le.fit_transform(df_encoded['smoker'])\n",
      "    df_encoded['region'] = le.fit_transform(df_encoded['region'])\n",
      "\n",
      "    # Split data into features and target\n",
      "    X = df_encoded.drop('charges', axis=1)\n",
      "    y = df_encoded['charges']\n",
      "\n",
      "    # Split data into training and testing sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "    # Initialize and train a random forest regressor\n",
      "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
      "    model.fit(X_train, y_train)\n",
      "\n",
      "    return model, X_train, X_test, y_train, y_test\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:53.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mrunning validate_step_51.py in /Users/ilya/Desktop/CodeGeneration-main/src/out_simple_case\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:53.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1m/Users/ilya/thesis_env/myenv/bin/python validate_step_51.py\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:54.231\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m240\u001b[0m - \u001b[34m\u001b[1mexit_code = 0\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:58.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlanguage_modeling\u001b[0m:\u001b[36mexecute_request\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mFull API response: {'id': 'gen-VhyDbW5JbqgH0O0yzqbecRIPHRNK', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1723661995, 'choices': [{'logprobs': None, 'finish_reason': 'eos', 'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the function definition for `step_61`:\\n```\\nimport pandas as pd\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error\\nimport numpy as np\\n\\ndef step_61(model, X_train, X_test, y_train, y_test):\\n    y_pred_train = model.predict(X_train)\\n    y_pred_test = model.predict(X_test)\\n    \\n    if y_train.dtype.kind in ['i', 'f']:  # regression task\\n        metrics = ['R^2', 'MSE', 'RMSE']\\n        evaluation_results = {\\n            'train': [r2_score(y_train, y_pred_train), mean_squared_error(y_train, y_pred_train), np.sqrt(mean_squared_error(y_train, y_pred_train))],\\n            'test': [r2_score(y_test, y_pred_test), mean_squared_error(y_test, y_pred_test), np.sqrt(mean_squared_error(y_test, y_pred_test))]\\n        }\\n    else:  # classification task\\n        metrics = ['accuracy', 'precision', 'recall', 'F1-score']\\n        evaluation_results = {\\n            'train': [accuracy_score(y_train, y_pred_train), precision_score(y_train, y_pred_train, average='weighted'), recall_score(y_train, y_pred_train, average='weighted'), f1_score(y_train, y_pred_train, average='weighted')],\\n            'test': [accuracy_score(y_test, y_pred_test), precision_score(y_test, y_pred_test, average='weighted'), recall_score(y_test, y_pred_test, average='weighted'), f1_score(y_test, y_pred_test, average='weighted')]\\n        }\\n    \\n    return evaluation_results, metrics\\n```\"}}], 'usage': {'prompt_tokens': 1122, 'completion_tokens': 354, 'total_tokens': 1476}}\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:58.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_step_file\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mgenerated step source for step 61, filename: step_61.py,\n",
      "source:\n",
      " \n",
      "import pandas as pd\n",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error\n",
      "import numpy as np\n",
      "\n",
      "def step_61(model, X_train, X_test, y_train, y_test):\n",
      "    y_pred_train = model.predict(X_train)\n",
      "    y_pred_test = model.predict(X_test)\n",
      "    \n",
      "    if y_train.dtype.kind in ['i', 'f']:  # regression task\n",
      "        metrics = ['R^2', 'MSE', 'RMSE']\n",
      "        evaluation_results = {\n",
      "            'train': [r2_score(y_train, y_pred_train), mean_squared_error(y_train, y_pred_train), np.sqrt(mean_squared_error(y_train, y_pred_train))],\n",
      "            'test': [r2_score(y_test, y_pred_test), mean_squared_error(y_test, y_pred_test), np.sqrt(mean_squared_error(y_test, y_pred_test))]\n",
      "        }\n",
      "    else:  # classification task\n",
      "        metrics = ['accuracy', 'precision', 'recall', 'F1-score']\n",
      "        evaluation_results = {\n",
      "            'train': [accuracy_score(y_train, y_pred_train), precision_score(y_train, y_pred_train, average='weighted'), recall_score(y_train, y_pred_train, average='weighted'), f1_score(y_train, y_pred_train, average='weighted')],\n",
      "            'test': [accuracy_score(y_test, y_pred_test), precision_score(y_test, y_pred_test, average='weighted'), recall_score(y_test, y_pred_test, average='weighted'), f1_score(y_test, y_pred_test, average='weighted')]\n",
      "        }\n",
      "    \n",
      "    return evaluation_results, metrics\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:58.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_validation_file\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mgenerated validation source for step 61, filename: validate_step_61.py,\n",
      "source:\n",
      " import pandas as pd\n",
      "from step_11 import step_11\n",
      "from step_21 import step_21\n",
      "from step_31 import step_31\n",
      "from step_41 import step_41\n",
      "from step_51 import step_51\n",
      "from step_61 import step_61\n",
      "\n",
      "\n",
      "csv_path = '/Users/ilya/Desktop/CodeGeneration-main/datasets/insurance.csv'\n",
      "\n",
      "def validate_step():\n",
      "    df = step_11(csv_path)\n",
      "    structure_info = step_21(df)\n",
      "    df_cleaned, data_types_info = step_31(df)\n",
      "    df_encoded = step_41(df_cleaned, data_types_info)\n",
      "    model, X_train, X_test, y_train, y_test = step_51(df_encoded)\n",
      "    evaluation_results, metrics = step_61(model, X_train, X_test, y_train, y_test)\n",
      "    print(evaluation_results, metrics)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    validate_step()\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:58.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mrunning validate_step_61.py in /Users/ilya/Desktop/CodeGeneration-main/src/out_simple_case\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:58.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1m/Users/ilya/thesis_env/myenv/bin/python validate_step_61.py\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:59.612\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m240\u001b[0m - \u001b[34m\u001b[1mexit_code = 0\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:59.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mgenerate_main_file\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mgenerated main source, filename: main.py,\n",
      "source:\n",
      " import pandas as pd\n",
      "from step_11 import step_11\n",
      "from step_21 import step_21\n",
      "from step_31 import step_31\n",
      "from step_41 import step_41\n",
      "from step_51 import step_51\n",
      "from step_61 import step_61\n",
      "\n",
      "csv_path = '/Users/ilya/Desktop/CodeGeneration-main/datasets/insurance.csv'\n",
      "\n",
      "def main():\n",
      "    df = step_11(csv_path)\n",
      "    structure_info = step_21(df)\n",
      "    df_cleaned, data_types_info = step_31(df)\n",
      "    df_encoded = step_41(df_cleaned, data_types_info)\n",
      "    model, X_train, X_test, y_train, y_test = step_51(df_encoded)\n",
      "    evaluation_results, metrics = step_61(model, X_train, X_test, y_train, y_test)\n",
      "    evaluation_results, metrics = step_61(model, X_train, X_test, y_train, y_test)\n",
      "    print(evaluation_results, metrics)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:59.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mrunning main.py in /Users/ilya/Desktop/CodeGeneration-main/src/out_simple_case\u001b[0m\n",
      "\u001b[32m2024-08-14 22:59:59.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1m/Users/ilya/thesis_env/myenv/bin/python main.py\u001b[0m\n",
      "\u001b[32m2024-08-14 23:00:00.494\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mvalidate_unit_code\u001b[0m:\u001b[36m240\u001b[0m - \u001b[34m\u001b[1mexit_code = 0\u001b[0m\n",
      "\u001b[32m2024-08-14 23:00:00.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36morchestrator\u001b[0m:\u001b[36mrun_steps\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mcode generated successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "orchestrator.run_steps(steps_simple_case, parameters_simple_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complicated case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below code for complicated case is commented out because it's not possible to provide a dataset for it since it's private. So, will not be able to run it. To test it you need to request the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = COMPLICATED_CASE_CSV_PATH\n",
    "# raw_data = pd.read_csv(csv_path)\n",
    "# dataset_info_compicated_case = get_dataset_info(raw_data)\n",
    "# signal_data = raw_data['signal'].values\n",
    "\n",
    "\n",
    "# SizeSegment = min(512, len(signal_data) // 100)\n",
    "# gamma = 'scale'\n",
    "# nu = 0.1\n",
    "# kernel = \"rbf\"\n",
    "\n",
    "# # PCA\n",
    "# pca = PCA().fit(signal_data.reshape(-1, 1))\n",
    "# cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "# NC_pca = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "# Dec_levels = int(np.log2(SizeSegment)) - 3\n",
    "\n",
    "# parameters_complicated_case = {\n",
    "#     'csv_path': f\"'{csv_path}'\",\n",
    "#     \"SizeSegment\": f\"{SizeSegment}\",\n",
    "#     \"gamma\": f\"'{gamma}'\",\n",
    "#     \"nu\": f\"{nu}\",\n",
    "#     \"kernel\" : f\"'{kernel}'\",\n",
    "#     \"NC_pca\": f\"{NC_pca}\",\n",
    "#     \"Dec_levels\": f\"{Dec_levels}\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_code_generator = main_code_generator = MainCodeGenerator([\n",
    "#         \"print(f'Precision on training data: {Prec_learn:.2f}')\",\n",
    "#         \"print(f'Precision on test data: {Prec_test:.2f}')\"\n",
    "# ])\n",
    "# prompt_generator = PromptGenerator(EXAMPLE_STEP_SCRIPT, dataset_info_compicated_case)\n",
    "\n",
    "# orchestrator = Orchestrator(\n",
    "#     model,\n",
    "#     prompt_generator,\n",
    "#     validation_code_genrator,\n",
    "#     main_code_generator,\n",
    "#     'out_complicated_case',\n",
    "#     PYTHON_PATH\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orchestrator.run_steps(steps_complicated_case, parameters_complicated_case)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

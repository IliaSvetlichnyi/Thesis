{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed for step: Choose appropriate machine learning algorithms based on the problem type\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/Unit_code_generator/step_7_code.py\", line 5, in <module>\n",
      "    X = pd.read_csv('/Users/ilya/Desktop/GitHub_Repositories/HW_University/Data_Mining/datasets/insurance.csv', \n",
      "NameError: name 'pd' is not defined. Did you mean: 'id'?\n",
      "\n",
      "Some code snippets failed validation. Documentation not generated.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Define the workflow steps with assigned numbers\n",
    "workflow_steps = {\n",
    "    11: \"Load the CSV file into a suitable format (e.g., DataFrame)\",\n",
    "    21: \"Examine the structure and characteristics of the data\",\n",
    "    22: \"Identify missing values, data types, and statistical summary\",\n",
    "    23: \"Visualize the data using charts, graphs, or plots\",\n",
    "    24: \"Gain insights and formulate hypotheses\",\n",
    "    31: \"Handle missing values (remove or impute)\",\n",
    "    32: \"Convert categorical variables to numerical representations\",\n",
    "    33: \"Perform feature scaling or normalization\",\n",
    "    34: \"Encode categorical variables (one-hot encoding, label encoding, etc.)\",\n",
    "    35: \"Split the data into training and testing sets\",\n",
    "    41: \"Create new features based on domain knowledge or data insights\",\n",
    "    42: \"Combine or transform existing features\",\n",
    "    43: \"Perform feature selection to identify relevant features\",\n",
    "    51: \"Choose appropriate machine learning algorithms based on the problem type\",\n",
    "    52: \"Define the model architecture and hyperparameters\",\n",
    "    53: \"Train the selected model on the training data\",\n",
    "    54: \"Utilize techniques like cross-validation for model evaluation\",\n",
    "    61: \"Evaluate the trained model's performance on the testing data\",\n",
    "    62: \"Calculate evaluation metrics (e.g., accuracy, precision, recall, F1-score)\",\n",
    "    63: \"Visualize the model's performance using confusion matrix, ROC curve, etc.\",\n",
    "    64: \"Fine-tune the model if necessary\",\n",
    "    71: \"Analyze the model's coefficients or feature importances\",\n",
    "    72: \"Visualize the model's decision boundaries or learned patterns\",\n",
    "    73: \"Interpret the model's predictions and explain its behavior\",\n",
    "    81: \"Generate unit code documentation during the code generation process\",\n",
    "    82: \"Execute the combined code and capture relevant outputs and insights\",\n",
    "    83: \"Create a comprehensive documentation for the entire workflow, including project overview, dataset details, selected steps, results, and interpretations\",\n",
    "    84: \"Present the documentation to users for understanding and reference\"\n",
    "}\n",
    "\n",
    "# Setup the prompt templates\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\"{request}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a highly skilled data scientist with 20 years of experience. You specialize in writing clean, efficient, and error-free ML code. Generate only the code snippets without any explanations or comments.\"),\n",
    "    human_prompt\n",
    "])\n",
    "\n",
    "model = Ollama(model=\"llama3\")\n",
    "\n",
    "# Function to generate code with LLM\n",
    "\n",
    "\n",
    "def generate_code_snippet(request):\n",
    "    formatted_request = chat_prompt.format_prompt(\n",
    "        request=request).to_messages()\n",
    "    response = model.invoke(formatted_request)\n",
    "    generated_code = response\n",
    "    return generated_code\n",
    "\n",
    "# Function to clean and correct the code\n",
    "\n",
    "\n",
    "def clean_and_correct_code(generated_code, csv_path):\n",
    "    cleaned_code = generated_code.replace(\"```\", \"\").strip()\n",
    "    cleaned_code_lines = cleaned_code.split(\"\\n\")\n",
    "    cleaned_code_lines = [\n",
    "        line for line in cleaned_code_lines if not line.lower().startswith(\"here is the\")]\n",
    "    cleaned_code = \"\\n\".join(cleaned_code_lines)\n",
    "    if \"python\" in cleaned_code:\n",
    "        cleaned_code = cleaned_code.split(\"python\")[1].strip()\n",
    "    corrected_code = cleaned_code.replace(\"{csv_path}\", f\"{csv_path}\")\n",
    "    return corrected_code\n",
    "\n",
    "# Load dataset information\n",
    "\n",
    "\n",
    "def get_dataset_info(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    columns = df.columns.tolist()\n",
    "    types = df.dtypes.to_dict()\n",
    "    sample_data = df.head().to_dict(orient='list')\n",
    "    value_counts = {col: df[col].value_counts().to_dict()\n",
    "                    for col in df.columns}\n",
    "    description = df.describe().to_dict()\n",
    "    return columns, types, sample_data, value_counts, description\n",
    "\n",
    "# Function to get selected workflow steps based on step numbers\n",
    "\n",
    "\n",
    "def get_selected_steps(step_numbers):\n",
    "    selected_steps = [workflow_steps[num]\n",
    "                      for num in step_numbers if num in workflow_steps]\n",
    "    return selected_steps\n",
    "\n",
    "# Function to validate a single unit code snippet\n",
    "\n",
    "\n",
    "def validate_unit_code(code_filename):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", code_filename], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(result.stderr)\n",
    "        return True, result.stdout\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Function to generate documentation for a step\n",
    "\n",
    "\n",
    "def generate_documentation(step, columns_info, types_info, sample_data_info, value_counts_info, description_info):\n",
    "    request = (\n",
    "        f\"Generate documentation for the following step: {step}. \"\n",
    "        f\"Provide insights, comments, and relevant information about the step without including graphs or complex visualizations. \"\n",
    "        f\"The dataset has the following columns: {columns_info}. \"\n",
    "        f\"The data types are: {types_info}. Sample data: {sample_data_info}. Value counts: {value_counts_info}. \"\n",
    "        f\"Description: {description_info}.\"\n",
    "    )\n",
    "    documentation = generate_code_snippet(request)\n",
    "    return documentation\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    csv_path = \"/Users/ilya/Desktop/GitHub_Repositories/HW_University/Data_Mining/datasets/insurance.csv\"\n",
    "    columns, types, sample_data, value_counts, description = get_dataset_info(\n",
    "        csv_path)\n",
    "\n",
    "    columns_info = \", \".join(columns)\n",
    "    types_info = \", \".join([f\"{col}: {typ}\" for col, typ in types.items()])\n",
    "    sample_data_info = \", \".join(\n",
    "        [f\"{col}: {vals[:5]}\" for col, vals in sample_data.items()])\n",
    "    value_counts_info = \", \".join(\n",
    "        [f\"{col}: {dict(list(vc.items())[:5])}\" for col, vc in value_counts.items()])\n",
    "    description_info = \", \".join(\n",
    "        [f\"{col}: {desc}\" for col, desc in description.items()])\n",
    "\n",
    "    # Hardcode the selected step numbers (replace with dynamic selection later)\n",
    "    selected_step_numbers = [11, 21, 22, 31, 32, 35, 51, 52, 53, 61, 62]\n",
    "\n",
    "    # Get selected workflow steps based on step numbers\n",
    "    selected_steps = get_selected_steps(selected_step_numbers)\n",
    "\n",
    "    # Generate and validate code snippets for each selected step\n",
    "    unit_code_filenames = []\n",
    "    documentation_snippets = []\n",
    "    combined_code = \"\"\n",
    "    for i, step in enumerate(selected_steps):\n",
    "        request = (\n",
    "            f\"Write a Python code snippet for the following step: {step}. \"\n",
    "            f\"Use placeholders like {csv_path} for dynamic inputs. The dataset has the following columns: {columns_info}. \"\n",
    "            f\"The data types are: {types_info}. Sample data: {sample_data_info}. Value counts: {value_counts_info}. \"\n",
    "            f\"Description: {description_info}. Only return the code without any explanations.\"\n",
    "        )\n",
    "        code_snippet = generate_code_snippet(request)\n",
    "        cleaned_code_snippet = clean_and_correct_code(code_snippet, csv_path)\n",
    "\n",
    "        code_filename = f\"step_{i+1}_code.py\"\n",
    "        with open(code_filename, \"w\") as file:\n",
    "            file.write(cleaned_code_snippet)\n",
    "\n",
    "        # Validate individual code snippets except for the last step\n",
    "        if i < len(selected_steps) - 1:\n",
    "            success, output = validate_unit_code(code_filename)\n",
    "            if success:\n",
    "                unit_code_filenames.append(code_filename)\n",
    "                documentation_snippet = generate_documentation(\n",
    "                    step, columns_info, types_info, sample_data_info, value_counts_info, description_info)\n",
    "                documentation_snippets.append(documentation_snippet)\n",
    "                combined_code += cleaned_code_snippet + \"\\n\\n\"\n",
    "            else:\n",
    "                print(f\"Validation failed for step: {step}\")\n",
    "                print(f\"Error: {output}\")\n",
    "                break\n",
    "        # Combine code snippets and validate for the last step (model training and evaluation)\n",
    "        else:\n",
    "            combined_code += cleaned_code_snippet + \"\\n\\n\"\n",
    "            with open(\"combined_code.py\", \"w\") as file:\n",
    "                file.write(combined_code)\n",
    "\n",
    "            success, output = validate_unit_code(\"combined_code.py\")\n",
    "            if success:\n",
    "                unit_code_filenames.append(code_filename)\n",
    "                documentation_snippet = generate_documentation(\n",
    "                    step, columns_info, types_info, sample_data_info, value_counts_info, description_info)\n",
    "                documentation_snippets.append(documentation_snippet)\n",
    "            else:\n",
    "                print(f\"Validation failed for step: {step}\")\n",
    "                print(f\"Error: {output}\")\n",
    "                break\n",
    "\n",
    "    # Save documentation to a separate file if all steps are validated successfully\n",
    "    if len(unit_code_filenames) == len(selected_steps):\n",
    "        with open(\"documentation.txt\", \"w\") as file:\n",
    "            file.write(\"\\n\".join(documentation_snippets))\n",
    "        print(\"Documentation saved to documentation.txt\")\n",
    "    else:\n",
    "        print(\"Some code snippets failed validation. Documentation not generated.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

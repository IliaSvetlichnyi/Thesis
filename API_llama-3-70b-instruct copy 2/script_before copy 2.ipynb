{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Tuple, Any, Set\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the workflow steps with assigned numbers and dependencies\n",
    "workflow_steps = {\n",
    "    11: {\n",
    "        \"description\": \"Load the CSV file as pandas DataFrame\",\n",
    "        \"dependencies\": []\n",
    "    },\n",
    "    21: {\n",
    "        \"description\": \"Examine the structure and characteristics of the data\",\n",
    "        \"dependencies\": [11]\n",
    "    },\n",
    "    22: {\n",
    "        \"description\": \"Identify missing values, data types, and statistical summary\",\n",
    "        \"dependencies\": [11, 21]\n",
    "    },\n",
    "    31: {\n",
    "        \"description\": \"Handle missing values (remove or impute) if there are so\",\n",
    "        \"dependencies\": [11, 22]\n",
    "    },\n",
    "    32: {\n",
    "        \"description\": \"Identify if there is a need to convert categorical variables to numerical representations. If yes, then convert them.\",\n",
    "        \"dependencies\": [11, 22, 31]\n",
    "    },\n",
    "    35: {\n",
    "        \"description\": \"Split the preprocessed data into training and testing sets\",\n",
    "        \"dependencies\": [11, 31, 32]\n",
    "    },\n",
    "    51: {\n",
    "        \"description\": \"Implement a single most appropriate machine learning algorithm for the dataset (choose from scikit-learn, XGBoost, LightGBM, or CatBoost).\",\n",
    "        \"dependencies\": [35]\n",
    "    },\n",
    "    52: {\n",
    "        \"description\": \"Fine-tune the model if necessary\",\n",
    "        \"dependencies\": [51]\n",
    "    },\n",
    "    53: {\n",
    "        \"description\": \"Train the selected model on the training data and evaluate its performance on the training data\",\n",
    "        \"dependencies\": [35, 51, 52]\n",
    "    },\n",
    "    61: {\n",
    "        \"description\": \"Evaluate the trained model's performance on the testing data\",\n",
    "        \"dependencies\": [35, 53]\n",
    "    },\n",
    "    62: {\n",
    "        \"description\": \"Calculate evaluation metrics (e.g., accuracy, precision, recall, F1-score)\",\n",
    "        \"dependencies\": [61]\n",
    "    }\n",
    "}\n",
    "\n",
    "# OpenAI API configuration\n",
    "api_url = \"https://openrouter.ai/api/v1\"\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "def openai_chat(request):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"meta-llama/llama-3-70b-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": request}]\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f\"{api_url}/chat/completions\", headers=headers, json=data)\n",
    "        response.raise_for_status()  # This will raise an exception for HTTP errors\n",
    "        result = response.json()\n",
    "        if 'choices' not in result or not result['choices']:\n",
    "            raise KeyError(\"No 'choices' in the API response\")\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"API request failed: {e}\")\n",
    "        print(f\"Response content: {response.text if 'response' in locals() else 'No response'}\")\n",
    "        raise\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"Unexpected API response format: {e}\")\n",
    "        print(f\"Response content: {response.text if 'response' in locals() else 'No response'}\")\n",
    "        raise\n",
    "\n",
    "def generate_code_snippet(request: str) -> str:\n",
    "    return openai_chat(request)\n",
    "\n",
    "def clean_and_correct_code(generated_code: str, csv_path: str) -> str:\n",
    "    cleaned_code = generated_code.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    cleaned_code_lines = [line for line in cleaned_code.split(\"\\n\") if not line.lower().startswith(\"here is the\")]\n",
    "    cleaned_code = \"\\n\".join(cleaned_code_lines)\n",
    "    corrected_code = cleaned_code.replace(\"{csv_path}\", f\"'{csv_path}'\")\n",
    "    return corrected_code\n",
    "\n",
    "def get_dataset_info(csv_path: str) -> Tuple[List[str], Dict[str, Any], Dict[str, List[Any]], Dict[str, Dict[Any, int]], Dict[str, Dict[str, float]]]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    columns = df.columns.tolist()\n",
    "    types = df.dtypes.to_dict()\n",
    "    sample_data = df.head().to_dict(orient='list')\n",
    "    value_counts = {col: df[col].value_counts().to_dict() for col in df.columns}\n",
    "    description = df.describe().to_dict()\n",
    "    return columns, types, sample_data, value_counts, description\n",
    "\n",
    "def save_dataset_info(csv_path: str, info_file_path: str):\n",
    "    columns, types, sample_data, value_counts, description = get_dataset_info(csv_path)\n",
    "    \n",
    "    # Convert types to strings as they're not JSON serializable\n",
    "    types = {k: str(v) for k, v in types.items()}\n",
    "    \n",
    "    # Limit the amount of data\n",
    "    limited_sample_data = {k: v[:5] for k, v in sample_data.items()}\n",
    "    limited_value_counts = {k: dict(list(v.items())[:5]) for k, v in value_counts.items()}\n",
    "    limited_description = {k: {sk: sv for sk, sv in v.items() if sk in ['count', 'mean', 'std', 'min', 'max']} for k, v in description.items()}\n",
    "    \n",
    "    dataset_info = {\n",
    "        'columns': columns,\n",
    "        'types': types,\n",
    "        'sample_data': limited_sample_data,\n",
    "        'value_counts': limited_value_counts,\n",
    "        'description': limited_description\n",
    "    }\n",
    "    \n",
    "    with open(info_file_path, 'w') as f:\n",
    "        json.dump(dataset_info, f)\n",
    "\n",
    "def validate_unit_code(code_filename: str) -> Tuple[bool, str]:\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", code_filename], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(result.stderr)\n",
    "        return True, result.stdout\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def generate_documentation(step: int, dataset_info: Dict[str, Any]) -> str:\n",
    "    request = (\n",
    "        f\"Provide a clear and concise description of the job performed by the code for the following step: {workflow_steps[step]['description']}. \"\n",
    "        f\"The description should summarize the main tasks and key points without going into the specifics of the code. \"\n",
    "        f\"The dataset has the following columns: {dataset_info['columns']}. \"\n",
    "        f\"The data types are: {dataset_info['types']}. \"\n",
    "        f\"Sample data: {dataset_info['sample_data']}. \"\n",
    "        f\"Value counts: {dataset_info['value_counts']}. \"\n",
    "        f\"Description: {dataset_info['description']}.\"\n",
    "    )\n",
    "    return generate_code_snippet(request)\n",
    "\n",
    "def fix_code(code_snippet: str, error_message: str, csv_path: str) -> str:\n",
    "    request = (\n",
    "        f\"The following code snippet encountered an error:\\n\\n{code_snippet}\\n\\n\"\n",
    "        f\"Error message:\\n{error_message}\\n\\n\"\n",
    "        f\"Please fix the code snippet to resolve the error without providing any explanations or comments.\"\n",
    "    )\n",
    "    fixed_code = generate_code_snippet(request)\n",
    "    return clean_and_correct_code(fixed_code, csv_path)\n",
    "\n",
    "def get_all_prerequisites(step: int, workflow_steps: Dict[int, Dict[str, Any]]) -> Set[int]:\n",
    "    prerequisites = set()\n",
    "    for prereq in workflow_steps[step]['dependencies']:\n",
    "        prerequisites.add(prereq)\n",
    "        prerequisites.update(get_all_prerequisites(prereq, workflow_steps))\n",
    "    return prerequisites\n",
    "\n",
    "def generate_code_for_step(step: int, workflow_steps: Dict[int, Dict[str, Any]], generated_code: Dict[int, str], csv_path: str, dataset_info: Dict[str, Any]) -> str:\n",
    "    prerequisites = get_all_prerequisites(step, workflow_steps)\n",
    "    \n",
    "    imports = [\"import pandas as pd\"]\n",
    "    for prereq in sorted(prerequisites):\n",
    "        imports.append(f\"from step_{prereq} import step_{prereq}\")\n",
    "    \n",
    "    function_code = generate_code_snippet_for_step(step, workflow_steps, csv_path, dataset_info)\n",
    "    \n",
    "    full_code = \"\\n\".join(imports) + \"\\n\\n\" + function_code\n",
    "    \n",
    "    # Save the code to a separate file\n",
    "    with open(f\"step_{step}.py\", \"w\") as file:\n",
    "        file.write(full_code)\n",
    "    \n",
    "    return full_code\n",
    "\n",
    "def generate_code_snippet_for_step(step: int, workflow_steps: Dict[int, Dict[str, Any]], csv_path: str, dataset_info: Dict[str, Any]) -> str:\n",
    "    request = (\n",
    "        f\"Write a Python function named 'step_{step}' for the following step: {workflow_steps[step]['description']}. \"\n",
    "        f\"The function should take a DataFrame 'df' as input and return the modified DataFrame. \"\n",
    "        f\"Ensure to include necessary imports and handle edge cases. \"\n",
    "        f\"The dataset has the following columns: {dataset_info['columns']}. \"\n",
    "        f\"The data types are: {dataset_info['types']}. \"\n",
    "        f\"Here's a sample of the data: {dataset_info['sample_data']}. \"\n",
    "        f\"Value counts (top 5): {dataset_info['value_counts']}. \"\n",
    "        f\"Statistical description: {dataset_info['description']}. \"\n",
    "        f\"Only return the function definition without any additional code or explanations.\"\n",
    "    )\n",
    "    code_snippet = generate_code_snippet(request)\n",
    "    return clean_and_correct_code(code_snippet, csv_path)\n",
    "\n",
    "def main():\n",
    "    csv_path = \"/Users/ilya/Desktop/GitHub_Repositories/HW_University/Data_Mining/datasets/insurance.csv\"\n",
    "    info_file_path = \"dataset_info.json\"\n",
    "    \n",
    "    if not os.path.exists(info_file_path):\n",
    "        save_dataset_info(csv_path, info_file_path)\n",
    "    \n",
    "    with open(info_file_path, 'r') as f:\n",
    "        dataset_info = json.load(f)\n",
    "    \n",
    "    selected_step_numbers = [11, 21, 22, 31, 32, 35, 51, 52, 53, 61, 62]\n",
    "    generated_code = {}\n",
    "    documentation_snippets = []\n",
    "\n",
    "    for step in selected_step_numbers:\n",
    "        try:\n",
    "            full_code = generate_code_for_step(step, workflow_steps, generated_code, csv_path, dataset_info)\n",
    "            \n",
    "            code_filename = f\"step_{step}.py\"\n",
    "            with open(code_filename, \"w\") as file:\n",
    "                file.write(full_code)\n",
    "            \n",
    "            success, output = validate_unit_code(code_filename)\n",
    "            while not success:\n",
    "                print(f\"Validation failed for step {step}: {output}\")\n",
    "                fixed_code = fix_code(full_code, output, csv_path)\n",
    "                with open(code_filename, \"w\") as file:\n",
    "                    file.write(fixed_code)\n",
    "                success, output = validate_unit_code(code_filename)\n",
    "            \n",
    "            generated_code[step] = full_code\n",
    "            documentation_snippet = generate_documentation(step, dataset_info)\n",
    "            documentation_snippets.append(documentation_snippet)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing step {step}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Generate the main script\n",
    "    main_script = \"import pandas as pd\\n\\n\"\n",
    "    for step in selected_step_numbers:\n",
    "        main_script += f\"from step_{step} import step_{step}\\n\"\n",
    "    \n",
    "    main_script += \"\\ndef main():\\n\"\n",
    "    main_script += f\"    csv_path = '{csv_path}'\\n\"\n",
    "    main_script += \"    df = pd.read_csv(csv_path)\\n\"\n",
    "    for step in selected_step_numbers:\n",
    "        main_script += f\"    df = step_{step}(df)\\n\"\n",
    "    \n",
    "    main_script += \"\\nif __name__ == '__main__':\\n\"\n",
    "    main_script += \"    main()\"\n",
    "\n",
    "    with open(\"main.py\", \"w\") as file:\n",
    "        file.write(main_script)\n",
    "\n",
    "    print(\"Modular code generated successfully.\")\n",
    "\n",
    "    # Validate the main script\n",
    "    success, output = validate_unit_code(\"main.py\")\n",
    "    if success:\n",
    "        print(\"Main script validated successfully.\")\n",
    "    else:\n",
    "        print(f\"Validation failed for main script.\")\n",
    "        print(f\"Error: {output}\")\n",
    "\n",
    "    # Save documentation to a separate file\n",
    "    with open(\"documentation.txt\", \"w\") as file:\n",
    "        file.write(\"\\n\\n\".join(documentation_snippets))\n",
    "    print(\"Documentation saved to documentation.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated code for step 11:\n",
      "\n",
      "def step_11(df):\n",
      "    # Assuming the input DataFrame 'df' is already loaded from the CSV file\n",
      "    # with the correct column names and data types\n",
      "    \n",
      "    # No modifications needed in this step, return the original DataFrame\n",
      "    return df\n",
      "\n",
      "DataFrame after step 11:\n",
      "   age     sex     bmi  children smoker     region      charges\n",
      "0   19  female  27.900         0    yes  southwest  16884.92400\n",
      "1   18    male  33.770         1     no  southeast   1725.55230\n",
      "2   28    male  33.000         3     no  southeast   4449.46200\n",
      "3   33    male  22.705         0     no  northwest  21984.47061\n",
      "4   32    male  28.880         0     no  northwest   3866.85520\n",
      "age           int64\n",
      "sex          object\n",
      "bmi         float64\n",
      "children      int64\n",
      "smoker       object\n",
      "region       object\n",
      "charges     float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Failed to execute step 21: string indices must be integers\n",
      "Skipping to the next step...\n",
      "Failed to execute step 22: string indices must be integers\n",
      "Skipping to the next step...\n",
      "Failed to execute step 31: string indices must be integers\n",
      "Skipping to the next step...\n",
      "Failed to execute step 32: string indices must be integers\n",
      "Skipping to the next step...\n",
      "Failed to execute step 35: string indices must be integers\n",
      "Skipping to the next step...\n",
      "Failed to execute step 51: string indices must be integers\n",
      "Skipping to the next step...\n",
      "Failed to execute step 52: string indices must be integers\n",
      "Skipping to the next step...\n",
      "Failed to execute step 53: string indices must be integers\n",
      "Skipping to the next step...\n",
      "Failed to execute step 61: string indices must be integers\n",
      "Skipping to the next step...\n",
      "Failed to execute step 62: string indices must be integers\n",
      "Skipping to the next step...\n",
      "Pipeline execution completed. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import json\n",
    "import pickle\n",
    "import textwrap\n",
    "import ast\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the workflow steps with assigned numbers and dependencies\n",
    "workflow_steps = {\n",
    "    11: {\n",
    "        \"description\": \"Load the CSV file as pandas DataFrame\",\n",
    "        \"dependencies\": []\n",
    "    },\n",
    "    21: {\n",
    "        \"description\": \"Examine the structure and characteristics of the data\",\n",
    "        \"dependencies\": [11]\n",
    "    },\n",
    "    22: {\n",
    "        \"description\": \"Identify missing values, data types, and statistical summary\",\n",
    "        \"dependencies\": [21]\n",
    "    },\n",
    "    31: {\n",
    "        \"description\": \"Handle missing values (remove or impute) if there are so\",\n",
    "        \"dependencies\": [22]\n",
    "    },\n",
    "    32: {\n",
    "        \"description\": \"Identify if there is a need to convert categorical variables to numerical representations. If yes, then convert them.\",\n",
    "        \"dependencies\": [31]\n",
    "    },\n",
    "    35: {\n",
    "        \"description\": \"Split the preprocessed data into training and testing sets\",\n",
    "        \"dependencies\": [32]\n",
    "    },\n",
    "    51: {\n",
    "        \"description\": \"Implement a single most appropriate machine learning algorithm for the dataset (choose from scikit-learn, XGBoost, LightGBM, or CatBoost).\",\n",
    "        \"dependencies\": [35]\n",
    "    },\n",
    "    52: {\n",
    "        \"description\": \"Fine-tune the model if necessary\",\n",
    "        \"dependencies\": [51]\n",
    "    },\n",
    "    53: {\n",
    "        \"description\": \"Train the selected model on the training data and evaluate its performance on the training data\",\n",
    "        \"dependencies\": [52]\n",
    "    },\n",
    "    61: {\n",
    "        \"description\": \"Evaluate the trained model's performance on the testing data\",\n",
    "        \"dependencies\": [53]\n",
    "    },\n",
    "    62: {\n",
    "        \"description\": \"Calculate evaluation metrics (e.g., accuracy, precision, recall, F1-score)\",\n",
    "        \"dependencies\": [61]\n",
    "    }\n",
    "}\n",
    "\n",
    "# OpenAI API configuration\n",
    "api_url = \"https://openrouter.ai/api/v1\"\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "class DataAnalysisPipeline:\n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.model = None\n",
    "        self.cache = {}\n",
    "        self.executed_steps = set()\n",
    "\n",
    "    def openai_chat(self, request: str) -> str:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        data = {\n",
    "            \"model\": \"meta-llama/llama-3-70b-instruct\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": request}]\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(f\"{api_url}/chat/completions\", headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            if 'choices' not in result or not result['choices']:\n",
    "                raise KeyError(\"No 'choices' in the API response\")\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"API request failed: {e}\")\n",
    "            print(f\"Response content: {response.text if 'response' in locals() else 'No response'}\")\n",
    "            raise\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Unexpected API response format: {e}\")\n",
    "            print(f\"Response content: {response.text if 'response' in locals() else 'No response'}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def validate_code(self, code: str) -> bool:\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            return True\n",
    "        except SyntaxError:\n",
    "            return False\n",
    "    \n",
    "    def generate_code_snippet(self, request: str) -> str:\n",
    "        return self.openai_chat(request)\n",
    "\n",
    "    def clean_and_correct_code(self, generated_code: str) -> str:\n",
    "        cleaned_code = generated_code.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "        cleaned_code_lines = [line for line in cleaned_code.split(\"\\n\") if not line.lower().startswith(\"here is the\")]\n",
    "        cleaned_code = \"\\n\".join(cleaned_code_lines)\n",
    "        return cleaned_code\n",
    "\n",
    "    def get_dataset_info(self) -> Dict[str, Any]:\n",
    "        if 11 not in self.cache:\n",
    "            self.df = pd.read_csv(self.csv_path)\n",
    "            columns = self.df.columns.tolist()\n",
    "            types = {k: str(v) for k, v in self.df.dtypes.to_dict().items()}\n",
    "            sample_data = self.df.head().to_dict(orient='list')\n",
    "            value_counts = {col: self.df[col].value_counts().to_dict() for col in self.df.columns}\n",
    "            description = self.df.describe().to_dict()\n",
    "            \n",
    "            self.cache[11] = {\n",
    "                'columns': columns,\n",
    "                'types': types,\n",
    "                'sample_data': sample_data,\n",
    "                'value_counts': value_counts,\n",
    "                'description': description\n",
    "            }\n",
    "        \n",
    "        return self.cache[11]\n",
    "\n",
    "    def summarize_dataset_info(self, dataset_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        summary = {\n",
    "            'columns': dataset_info['columns'],\n",
    "            'types': dataset_info['types'],\n",
    "            'sample_data': {k: v[:3] for k, v in dataset_info['sample_data'].items()},\n",
    "            'value_counts': {k: dict(list(v.items())[:3]) for k, v in dataset_info['value_counts'].items()},\n",
    "            'description': {k: {sk: sv for sk, sv in v.items() if sk in ['count', 'mean', 'min', 'max']} \n",
    "                            for k, v in dataset_info['description'].items()}\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "    def generate_code_for_step(self, step: int) -> str:\n",
    "        dataset_info = self.get_dataset_info()\n",
    "        summarized_info = self.summarize_dataset_info(dataset_info)\n",
    "        \n",
    "        base_request = (\n",
    "            f\"Write a Python function named 'step_{step}' that takes a pandas DataFrame 'df' as input and returns the modified DataFrame. \"\n",
    "            f\"The function should perform the following step: {workflow_steps[step]['description']}. \"\n",
    "            f\"The dataset has the following columns: {summarized_info['columns']}. \"\n",
    "            f\"The data types are: {summarized_info['types']}. \"\n",
    "            f\"Here's a sample of the data: {summarized_info['sample_data']}. \"\n",
    "            f\"The current state of the DataFrame is:\\n{self.df.head() if self.df is not None else 'Not loaded yet'}\\n\"\n",
    "            f\"Only return the function definition without any additional explanations. \"\n",
    "            f\"Use the actual column names from the dataset in your code.\"\n",
    "        )\n",
    "\n",
    "        # Split the request into smaller chunks if it's too large\n",
    "        chunks = textwrap.wrap(base_request, width=4000)  # Adjust width as needed\n",
    "        code_snippets = []\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i == 0:\n",
    "                chunk_request = chunk + \"\\nProvide the first part of the function.\"\n",
    "            elif i == len(chunks) - 1:\n",
    "                chunk_request = chunk + \"\\nComplete the function.\"\n",
    "            else:\n",
    "                chunk_request = chunk + \"\\nContinue the function.\"\n",
    "\n",
    "            code_snippet = self.generate_code_snippet(chunk_request)\n",
    "            code_snippets.append(self.clean_and_correct_code(code_snippet))\n",
    "\n",
    "        full_code = \"\\n\".join(code_snippets)\n",
    "        return full_code\n",
    "\n",
    "    def execute_step(self, step: int) -> None:\n",
    "        if step in self.executed_steps:\n",
    "            return\n",
    "\n",
    "        for prereq in workflow_steps[step]['dependencies']:\n",
    "            self.execute_step(prereq)\n",
    "\n",
    "        code = self.generate_code_for_step(step)\n",
    "        print(f\"Generated code for step {step}:\\n{code}\\n\")\n",
    "        \n",
    "        if not self.validate_code(code):\n",
    "            print(f\"Invalid code generated for step {step}. Attempting to fix...\")\n",
    "            code = self.fix_code(code, step)\n",
    "        \n",
    "        try:\n",
    "            exec(code, globals())\n",
    "            step_function = globals()[f\"step_{step}\"]\n",
    "            \n",
    "            if step == 35:  # Split data step\n",
    "                self.X_train, self.X_test, self.y_train, self.y_test = step_function(self.df)\n",
    "            elif step > 35:  # Model-related steps\n",
    "                result = step_function(self.X_train, self.X_test, self.y_train, self.y_test, self.model)\n",
    "                if isinstance(result, tuple):\n",
    "                    self.model, metrics = result\n",
    "                else:\n",
    "                    self.model = result\n",
    "            else:\n",
    "                self.df = step_function(self.df)\n",
    "            \n",
    "            print(f\"DataFrame after step {step}:\")\n",
    "            print(self.df.head())\n",
    "            print(self.df.dtypes)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            self.cache[step] = code\n",
    "            self.executed_steps.add(step)\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing step {step}: {str(e)}\")\n",
    "            print(\"Attempting to fix the code...\")\n",
    "            fixed_code = self.fix_code(code, step, str(e))\n",
    "            self.execute_step(step)  # Recursively try again with fixed code\n",
    "\n",
    "    def fix_code(self, code: str, step: int, error_message: str = \"\") -> str:\n",
    "        request = (\n",
    "            f\"The following Python code for step {step} has an error:\\n\\n{code}\\n\\n\"\n",
    "            f\"Error message: {error_message}\\n\"\n",
    "            \"Please fix the code and provide only the corrected function without any explanations.\"\n",
    "        )\n",
    "        fixed_code = self.generate_code_snippet(request)\n",
    "        return self.clean_and_correct_code(fixed_code)\n",
    "\n",
    "    def run_pipeline(self, steps: List[int]) -> None:\n",
    "        for step in steps:\n",
    "            try:\n",
    "                self.execute_step(step)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to execute step {step}: {str(e)}\")\n",
    "                print(\"Skipping to the next step...\")\n",
    "\n",
    "    def get_combined_code(self) -> str:\n",
    "        return \"\\n\\n\".join([self.cache[step] for step in sorted(self.executed_steps)])\n",
    "\n",
    "    def save_results(self) -> None:\n",
    "        with open(\"combined_code.py\", \"w\") as f:\n",
    "            f.write(self.get_combined_code())\n",
    "        \n",
    "        with open(\"model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.model, f)\n",
    "        \n",
    "        results = {\n",
    "            \"X_train\": self.X_train.to_dict() if self.X_train is not None else None,\n",
    "            \"X_test\": self.X_test.to_dict() if self.X_test is not None else None,\n",
    "            \"y_train\": self.y_train.to_list() if self.y_train is not None else None,\n",
    "            \"y_test\": self.y_test.to_list() if self.y_test is not None else None,\n",
    "        }\n",
    "        with open(\"results.json\", \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "def main():\n",
    "    csv_path = \"/Users/ilya/Desktop/GitHub_Repositories/HW_University/Data_Mining/datasets/insurance.csv\"\n",
    "    pipeline = DataAnalysisPipeline(csv_path)\n",
    "    steps_to_execute = [11, 21, 22, 31, 32, 35, 51, 52, 53, 61, 62]\n",
    "    pipeline.run_pipeline(steps_to_execute)\n",
    "    pipeline.save_results()\n",
    "    print(\"Pipeline execution completed. Results saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

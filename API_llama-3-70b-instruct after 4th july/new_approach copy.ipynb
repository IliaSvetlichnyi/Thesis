{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the Graph with Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_steps = {\n",
    "    11: {\n",
    "        \"description\": \"Load the CSV file as pandas DataFrame\",\n",
    "        \"dependencies\": [],\n",
    "        \"input\": [\"csv_path\"],\n",
    "        \"output\": [\"df\"]\n",
    "    },\n",
    "    21: {\n",
    "        \"description\": \"Examine the structure and characteristics of the data\",\n",
    "        \"dependencies\": [11],\n",
    "        \"input\": [\"df\"],\n",
    "        \"output\": [\"structure_info\"]\n",
    "    },\n",
    "    31: {\n",
    "        \"description\": \"Identify missing values, data types, and handle missing values if there are any\",\n",
    "        \"dependencies\": [11, 21],\n",
    "        \"input\": [\"df\"],\n",
    "        \"output\": [\"df_cleaned\", \"data_types_info\"]\n",
    "    },\n",
    "    32: {\n",
    "        \"description\": \"Identify if there is a need to convert categorical variables to numerical representations. If yes, then convert them.\",\n",
    "        \"dependencies\": [11, 31],\n",
    "        \"input\": [\"df_cleaned\", \"data_types_info\"],\n",
    "        \"output\": [\"df_encoded\"]\n",
    "    },\n",
    "    51: {\n",
    "        \"description\": \"Split the preprocessed data into training and testing sets, and implement a machine learning algorithm (choose from scikit-learn, XGBoost, LightGBM, or CatBoost).\",\n",
    "        \"dependencies\": [11, 31, 32],\n",
    "        \"input\": [\"df_encoded\"],\n",
    "        \"output\": [\"model\", \"X_train\", \"X_test\", \"y_train\", \"y_test\"]\n",
    "    },\n",
    "    61: {\n",
    "        \"description\": \"Evaluate the model's performance on both training and testing data, calculate evaluation metrics (for classification: [accuracy, precision, recall, F1-score]; for regression: [R^2, MSE, RMSE]), and compare the difference.\",\n",
    "        \"dependencies\": [51],\n",
    "        \"input\": [\"model\", \"X_train\", \"X_test\", \"y_train\", \"y_test\"],\n",
    "        \"output\": [\"evaluation_results\", \"metrics\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Validation Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full API response: {'id': 'gen-cBGX5CJUB3FRxJaJrEHm16pWtoLK', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324318, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```\\nimport pandas as pd\\n\\ndef step_11(csv_path):\\n    try:\\n        df = pd.read_csv(csv_path)\\n        return df\\n    except FileNotFoundError:\\n        print(\"The file was not found. Please check the path.\")\\n        return None\\n    except pd.errors.EmptyDataError:\\n        print(\"The file is empty. Please check the file content.\")\\n        return None\\n    except pd.errors.ParserError:\\n        print(\"Error parsing the file. Please check the file format.\")\\n        return None\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 983, 'completion_tokens': 104, 'total_tokens': 1087}}\n",
      "Full API response: {'id': 'gen-XpDmJWIHQ1ehQ8x8n25t8PYO2rMn', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324324, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the `step_21` function:\\n```\\nimport pandas as pd\\n\\ndef step_21(df):\\n    structure_info = {\\n        'columns': df.columns.tolist(),\\n        'data_types': df.dtypes.to_dict(),\\n        'value_counts': {col: df[col].value_counts().head(5).to_dict() for col in df.columns},\\n        'statistical_description': df.describe().to_dict()\\n    }\\n    return structure_info\\n```\"}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 985, 'completion_tokens': 95, 'total_tokens': 1080}}\n",
      "Full API response: {'id': 'gen-shFjkriKsjkf2JDgqdMba3ya8vNu', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324329, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '```\\nimport pandas as pd\\nimport numpy as np\\n\\ndef step_31(df):\\n    df_cleaned = df.copy()\\n    data_types_info = df_cleaned.dtypes.apply(lambda x: x.name).to_dict()\\n    \\n    # Identify missing values\\n    missing_values = df_cleaned.isnull().sum()\\n    \\n    # Handle missing values\\n    df_cleaned.fillna(df_cleaned.mean(), inplace=True)\\n    \\n    return df_cleaned, data_types_info\\n```'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 996, 'completion_tokens': 99, 'total_tokens': 1095}}\n",
      "Validation failed for step 31: Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_31.py\", line 14, in <module>\n",
      "    validate_step()\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/validate_step_31.py\", line 10, in validate_step\n",
      "    df_cleaned, data_types_info = step_31(df)\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct after 4th july/step_31.py\", line 12, in step_31\n",
      "    df_cleaned.fillna(df_cleaned.mean(), inplace=True)\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/frame.py\", line 11680, in mean\n",
      "    result = super().mean(axis, skipna, numeric_only, **kwargs)\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/generic.py\", line 12417, in mean\n",
      "    return self._stat_function(\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/generic.py\", line 12374, in _stat_function\n",
      "    return self._reduce(\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/frame.py\", line 11549, in _reduce\n",
      "    res = df._mgr.reduce(blk_func)\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/internals/managers.py\", line 1500, in reduce\n",
      "    nbs = blk.reduce(func)\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/internals/blocks.py\", line 404, in reduce\n",
      "    result = func(self.values)\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/frame.py\", line 11468, in blk_func\n",
      "    return op(values, axis=axis, skipna=skipna, **kwds)\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/nanops.py\", line 147, in f\n",
      "    result = alt(values, axis=axis, skipna=skipna, **kwds)\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/nanops.py\", line 404, in new_func\n",
      "    result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/nanops.py\", line 720, in nanmean\n",
      "    the_sum = _ensure_numeric(the_sum)\n",
      "  File \"/Users/ilya/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/nanops.py\", line 1686, in _ensure_numeric\n",
      "    raise TypeError(f\"Could not convert {x} to numeric\")\n",
      "TypeError: Could not convert ['femalemalemalemalemalefemalefemalefemalemalefemalemalefemalemalefemalemalemalefemalemalemalemalefemalefemalemalefemalemalefemalefemalefemalemalemalemalefemalefemalemalemalemalefemalemalemalemalefemalefemalemalefemalemalemalefemalefemalefemalemalefemalefemalemalemalefemalemalefemalemalefemalefemalemalemalemalefemalefemalefemalefemalemalefemalemalefemalemalefemalemalemalemalefemalemalefemalefemalemalefemalemalefemalefemalemalefemalefemalefemalefemalefemalefemalemalemalefemalefemalefemalemalemalemalefemalemalefemalefemalefemalemalefemalemalemalemalemalefemalemalefemalemalemalemalefemalefemalefemalemalemalefemalemalefemalefemalefemalefemalefemalemalefemalefemalefemalemalefemalefemalemalemalefemalefemalemalemalemalemalemalefemalemalefemalefemalemalemalemalefemalefemalefemalemalemalemalemalefemalefemalefemalemalefemalemalemalefemalefemalefemalemalemalemalemalefemalefemalefemalemalemalefemalefemalemalefemalemalefemalemalemalefemalefemalefemalefemalemalefemalemalefemalemalemalefemalefemalefemalefemalefemalefemalefemalefemalemalefemalemalemalefemalemalemalemalemalefemalefemalefemalefemalemalefemalefemalefemalefemalemalemalemalemalemalefemalefemalemalefemalefemalefemalemalemalefemalefemalemalemalemalefemalefemalefemalemalefemalemalefemalemalemalemalemalefemalemalemalemalefemalemalefemalemalemalefemalefemalemalemalefemalemalemalefemalemalemalemalemalemalemalemalefemalemalefemalemalefemalefemalemalemalefemalefemalemalefemalefemalefemalemalefemalemalemalefemalemalemalemalemalemalefemalemalefemalefemalefemalefemalemalefemalefemalemalefemalemalefemalemalemalefemalemalemalemalefemalemalemalefemalemalemalemalemalefemalemalefemalemalefemalemalefemalefemalefemalemalemalemalemalefemalefemalemalefemalemalefemalefemalemalemalefemalemalefemalefemalefemalemalefemalemalemalemalemalefemalefemalemalefemalefemalefemalefemalefemalefemalemalemalefemalefemalefemalemalemalefemalefemalemalefemalemalefemalemalemalefemalemalemalefemalemalefemalefemalemalefemalemalemalefemalemalefemalemalemalefemalefemalemalefemalemalemalefemalefemalefemalemalemalemalefemalefemalemalefemalefemalemalefemalemalefemalemalemalemalemalemalemalefemalefemalefemalefemalemalefemalemalefemalemalemalemalemalefemalemalefemalefemalemalefemalemalefemalemalefemalefemalemalemalemalemalemalemalemalefemalefemalemalefemalefemalemalefemalemalemalefemalefemalefemalefemalefemalemalefemalefemalefemalemalemalemalemalemalemalemalemalefemalefemalemalefemalefemalemalefemalemalefemalefemalefemalemalemalemalefemalemalefemalefemalemalemalemalemalefemalemalemalemalefemalefemalemalemalemalemalemalemalemalemalefemalemalefemalefemalefemalefemalemalefemalefemalefemalemalemalemalefemalemalemalemalemalefemalefemalefemalemalefemalefemalefemalefemalemalemalemalefemalefemalefemalemalefemalemalefemalefemalemalemalemalefemalemalefemalefemalemalemalefemalefemalefemalemalefemalemalefemalefemalefemalefemalefemalefemalemalefemalemalefemalemalemalemalefemalemalefemalemalefemalefemalefemalefemalemalemalefemalemalefemalefemalefemalemalefemalefemalemalefemalefemalefemalefemalefemalefemalemalemalefemalefemalefemalefemalefemalefemalefemalemalefemalefemalemalemalefemalemalemalefemalemalemalemalefemalemalemalefemalemalemalemalefemalefemalemalemalemalemalemalefemalemalemalemalefemalemalefemalefemalefemalefemalefemalefemalefemalefemalemalefemalefemalemalefemalefemalemalefemalemalemalefemalemalefemalemalefemalemalefemalefemalemalefemalemalemalefemalefemalemalemalemalefemalemalemalemalefemalemalemalemalemalemalefemalefemalefemalemalemalefemalefemalefemalemalefemalefemalefemalefemalemalefemalefemalemalefemalefemalemalefemalemalefemalemalefemalefemalefemalemalemalemalefemalefemalemalefemalefemalefemalemalemalefemalefemalefemalefemalefemalemalemalemalemalemalemalefemalemalefemalemalemalefemalemalefemalemalemalefemalemalemalefemalefemalemalemalefemalemalemalemalefemalefemalemalefemalefemalefemalemalefemalefemalefemalemalemalemalemalemalemalemalemalemalefemalefemalefemalemalemalemalefemalefemalemalefemalemalefemalemalemalefemalefemalemalefemalefemalemalefemalemalefemalefemalefemalemalemalefemalefemalemalemalemalefemalefemalemalefemalefemalemalemalefemalefemalemalefemalemalemalemalemalemalefemalefemalemalemalemalemalefemalefemalefemalemalemalefemalefemalemalefemalefemalemalefemalemalefemalemalefemalefemalefemalefemalefemalemalefemalemalefemalefemalefemalefemalemalemalemalemalemalefemalemalefemalemalemalemalefemalefemalemalemalefemalemalemalefemalefemalemalemalemalefemalemalemalefemalefemalemalemalemalefemalefemalemalefemalefemalemalemalemalemalefemalefemalemalefemalemalefemalemalemalefemalefemalemalefemalefemalemalefemalefemalefemalefemalemalemalemalemalefemalefemalefemalemalemalefemalemalefemalemalefemalemalefemalemalemalemalefemalefemalemalemalefemalemalemalemalemalemalemalefemalemalemalemalemalemalemalemalefemalefemalefemalemalemalemalemalemalemalefemalefemalefemalefemalefemalemalemalemalemalefemalefemalemalemalemalefemalemalefemalemalefemalemalefemalefemalefemalefemalemalefemalefemalefemalefemalefemalefemalemalemalemalemalemalemalemalemalemalemalefemalemalefemalemalefemalemalefemalefemalefemalefemalemalefemalemalemalefemalefemalemalemalemalefemalefemalefemalefemalemalemalefemalemalefemalemalemalefemalemalemalefemalemalefemalefemalemalefemalemalefemalemalemalemalefemalemalefemalefemalefemalemalemalemalemalemalefemalefemalemalemalemalefemalemalemalemalefemalefemalefemalefemalemalemalemalemalemalemalemalefemalefemalemalemalemalemalemalefemalemalefemalefemalefemalefemalemalefemalefemalefemalemalemalemalemalefemalefemalefemalemalemalefemalemalefemalefemalemalemalemalemalemalefemalefemalemalefemalefemalefemalefemalemalefemalemalefemalefemalemalemalefemalemalefemalefemalefemalemalefemalemalefemalefemalemalemalemalemalefemalemalemalefemalefemalefemalefemalefemalefemalemalefemalefemalefemalefemalemalemalefemalefemalefemalemalefemalemalefemalemalefemalefemalemalemalefemalefemalefemalefemalemalefemalefemalefemalefemalefemalemalemalefemalefemalefemalefemalefemalefemalefemalefemalefemalefemalemalemalefemalemalemalemalemalefemalemalefemalemalefemalemalemalemalemalefemalefemalemalemalemalefemalefemalefemalemalemalefemalemalefemalemalemalemalemalemalefemalefemalemalefemalemalefemalefemalemalefemalemalemalefemalefemalemalemalefemalemalefemalemalemalemalemalefemalefemalefemalefemalefemalemalefemalefemalemalefemalefemalefemalemalefemalemalefemalemalemalefemalemalemalemalemalefemalefemalemalefemalefemalefemalefemalemalemalefemalefemalefemalemalemalefemalemalemalemalemalemalemalefemalemalefemalemalemalefemalemalemalefemalefemalemalefemalemalemalefemalemalefemalefemalemalefemalemalemalefemalemalemalemalefemalemalemalefemalemalefemalemalefemalefemalefemalemalefemalefemalefemalefemale'\n",
      " 'yesnonononononononononoyesnonoyesnonononoyesnononoyesnononononoyesyesnononoyesnononoyesyesnononononononononoyesnonoyesyesnoyesnoyesyesnononononoyesnonononoyesyesnononononononononononoyesnoyesyesyesnononononoyesnoyesnononoyesyesnononoyesnoyesnononoyesnononononononoyesnononononoyesnonoyesnoyesnononononononononononononoyesnoyesnoyesnonononononoyesnonoyesyesyesnoyesyesnononononononononononononoyesnononononononononoyesnononononononononononononononononoyesnononoyesnononononononononononononononoyesyesnonononononononononoyesnonoyesnoyesnononoyesnononononoyesyesyesnoyesnoyesnonoyesnoyesyesyesnoyesyesnonononoyesnonononononononoyesyesnonononononoyesnononoyesnononoyesyesyesnonoyesnonononononononononoyesnoyesnononononononoyesnonononoyesyesnoyesyesnonononononoyesnononononononononononononononononononononononoyesnonononononononononoyesnoyesyesyesnonoyesyesnononononononononononononononononononononononononononononoyesyesnonononoyesnoyesyesyesyesnonononononononononononononononononoyesnonoyesnonononononononononononononononoyesnononoyesnonononononononoyesyesyesnononononononononononoyesnononononoyesnononononoyesnoyesyesnonononononononononoyesnononononononononoyesnononononoyesnonononononononononononoyesnoyesnononoyesnonononononononoyesnonononononononononoyesnononononononoyesnononononononononoyesnononononoyesnonononononononononoyesnonoyesnoyesnononononoyesnoyesyesnonoyesnoyesnononononoyesnonononononononoyesnonoyesnononononononononononononoyesnonononononononoyesyesnoyesyesnononononoyesnonoyesnonononoyesnonononononoyesnononononononoyesnonononononononoyesnonononononononononononononononononoyesnoyesnonoyesnononononoyesnoyesyesnoyesyesnononononononoyesnonononononoyesnoyesnonoyesnonononononononononoyesnonononononoyesnonoyesnononononononononoyesnoyesnononoyesnononoyesnonononononononononononononoyesyesnonononononoyesyesyesnononononononononononononoyesyesnoyesnonononoyesnoyesnoyesnoyesyesnonoyesnonononononononononononononononononononononoyesnoyesyesnononoyesnonoyesnonoyesnonononoyesnononononononoyesnoyesnonononoyesyesnononononononononononononononononononononononononononononoyesnoyesnoyesnoyesyesnoyesnoyesnononononononoyesnonononononononoyesnonononononoyesnonononononoyesnonononoyesnononononoyesyesnononononoyesnononoyesnononononononononoyesyesnononoyesnononoyesyesnoyesnonoyesyesnonoyesnoyesnonoyesnoyesnoyesnononoyesnonononononononoyesnononononononoyesnononononononoyesnonononononoyesnonononoyesnonoyesnonoyesnononoyesnonononononononononoyesyesnonononoyesyesnoyesnoyesnoyesnonononononononononononononoyesnonononononoyesnononononoyesnononoyesnononononononononononononoyesyesnonononoyesnonoyesnonononoyesnoyesnoyesnononononononoyesnononononononoyesnonoyesyesnononononononononoyesnonononoyesnonononononoyesyesnonononononononoyesyesnononononononoyesyesnoyesnonononononononononononoyesnoyesnonononononoyesnononoyesnonoyesyesnoyesnononoyesnonoyesnonononononononoyesyesnoyesyesnoyesyesyesnonononoyesyesnonononononoyesnoyesnononononononononononononoyes'\n",
      " 'southwestsoutheastsoutheastnorthwestnorthwestsoutheastsoutheastnorthwestnortheastnorthwestnortheastsoutheastsouthwestsoutheastsoutheastsouthwestnortheastnortheastsouthwestsouthwestnortheastsouthwestsoutheastnortheastnorthwestsoutheastnortheastnorthwestnorthwestsouthwestsouthwestnortheastsouthwestnorthwestsouthwestnorthwestnorthwestsouthwestnortheastsouthwestnortheastsoutheastsoutheastsoutheastnortheastsouthwestnortheastnorthwestsoutheastsoutheastnortheastnorthwestsouthwestsoutheastnorthwestnorthwestnortheastsoutheastsoutheastnorthwestnortheastsoutheastnorthwestnorthwestnorthwestsouthwestsouthwestnorthwestsoutheastsoutheastsoutheastnortheastsouthwestsoutheastsouthwestnorthwestsoutheastsoutheastnortheastnorthwestnortheastnortheastsoutheastnorthwestsouthwestnorthwestnorthwestsouthwestnorthwestnorthwestsoutheastnorthwestnortheastnorthwestsouthwestsoutheastsouthwestsoutheastnortheastsouthwestsouthwestnortheastnortheastsoutheastsouthwestnorthwestsouthwestnorthwestsoutheastsoutheastnorthwestsouthwestsouthwestnorthwestnortheastnortheastsoutheastsoutheastsoutheastnorthwestsouthwestnortheastnorthwestnortheastnorthwestnortheastsouthwestsouthwestnorthwestsouthwestnortheastnortheastsouthwestnorthwestnortheastsoutheastsouthwestnorthwestsoutheastsouthwestnortheastnortheastsoutheastnorthwestnorthwestsoutheastnorthwestsoutheastnorthwestsouthwestnorthwestsoutheastnortheastnortheastnortheastnorthwestsoutheastnortheastsoutheastsoutheastnorthwestsoutheastsouthwestsouthwestnorthwestnortheastsouthwestnorthwestnorthwestnortheastsoutheastsouthwestnortheastsouthwestnorthwestsouthwestnorthwestsouthwestsouthwestnortheastnorthwestsoutheastnortheastnorthwestsoutheastnortheastsoutheastsouthwestsouthwestnorthwestsoutheastsouthwestsoutheastnorthwestsoutheastnorthwestsouthwestsoutheastnorthwestnortheastnorthwestsoutheastnorthwestsoutheastsouthwestnortheastsoutheastnortheastsouthwestnortheastsouthwestnorthwestnorthwestsoutheastsouthwestsouthwestnorthwestsoutheastsoutheastsoutheastsouthwestnortheastsouthwestsouthwestsoutheastsoutheastsoutheastsoutheastnortheastnortheastnorthwestsoutheastsouthwestsouthwestnorthwestsoutheastsoutheastsoutheastnorthwestsoutheastnortheastnortheastsouthwestsouthwestnortheastnorthwestsoutheastsoutheastsouthwestnortheastnortheastsouthwestsoutheastsouthwestnortheastnortheastnorthwestsoutheastnorthwestnorthwestsouthwestsoutheastnortheastnorthwestsoutheastsoutheastsoutheastnortheastsouthwestnortheastsoutheastsouthwestnorthwestnortheastnorthwestnortheastnorthwestsouthwestsoutheastsoutheastnortheastnortheastnortheastnortheastsouthwestsoutheastnortheastnorthwestnortheastsoutheastsouthwestnortheastsoutheastsoutheastsouthwestnortheastsouthwestsoutheastnorthwestnorthwestnortheastnortheastsoutheastsoutheastsouthwestnorthwestsouthwestsoutheastnortheastnorthwestsouthwestsouthwestsoutheastsoutheastsouthwestnortheastnorthwestnortheastnorthwestnortheastnorthwestnortheastsouthwestnortheastsouthwestnortheastsoutheastnorthwestsouthwestsouthwestnortheastnorthwestnorthwestnortheastnortheastsouthwestsoutheastnorthwestnortheastsoutheastsouthwestnorthwestnortheastnortheastsoutheastsoutheastsoutheastnortheastsoutheastnorthwestnorthwestsouthwestsouthwestnortheastsoutheastsouthwestsoutheastnorthwestsoutheastsoutheastnortheastsouthwestsouthwestsouthwestsoutheastnortheastnortheastnorthwestnorthwestnortheastnorthwestnortheastnortheastsouthwestsoutheastnorthwestnortheastsoutheastnorthwestsoutheastnortheastnortheastsoutheastsoutheastnortheastsouthwestsoutheastnorthwestnorthwestnorthwestnortheastnorthwestnortheastnortheastnortheastnorthwestsouthwestsoutheastsouthwestsoutheastsouthwestsoutheastnorthwestnorthwestsouthwestnortheastsoutheastsouthwestsoutheastsoutheastnorthwestnortheastnortheastsouthwestnorthwestsoutheastsoutheastsouthwestsoutheastnorthwestsoutheastsoutheastnortheastnortheastsouthwestsoutheastnortheastnortheastnortheastnorthwestsouthwestnorthwestsouthwestsouthwestnorthwestsoutheastnortheastsouthwestsoutheastnortheastnorthwestsouthwestsoutheastsoutheastnorthwestsouthwestnortheastnorthwestsouthwestsouthwestsouthwestnorthwestsouthwestnorthwestsoutheastsouthwestsoutheastnorthwestsouthwestsoutheastsoutheastsouthwestnortheastnortheastnorthwestsoutheastsouthwestnorthwestnortheastsoutheastsoutheastnortheastsouthwestnortheastsouthwestnorthwestnortheastnorthwestsoutheastsoutheastnorthwestsoutheastsoutheastsouthwestsouthwestnortheastnorthwestsouthwestsoutheastnorthwestsouthwestsoutheastnortheastsouthwestsouthwestnortheastsouthwestsouthwestsoutheastsouthwestsouthwestnortheastsoutheastsoutheastsoutheastnorthwestnorthwestnorthwestnortheastsouthwestnortheastsoutheastnortheastsouthwestsouthwestsouthwestsoutheastnorthwestsouthwestnortheastnortheastsoutheastnortheastsoutheastsoutheastsoutheastnorthwestsouthwestnortheastnortheastsoutheastnortheastsoutheastsoutheastsoutheastnortheastsouthwestsouthwestsoutheastsoutheastsouthwestsoutheastsoutheastsoutheastnorthwestnorthwestnortheastsouthwestnortheastsoutheastsouthwestsoutheastsouthwestnorthwestnortheastsouthwestnortheastsoutheastnorthwestnorthwestnorthwestnortheastsouthwestsoutheastsoutheastnorthwestnorthwestnorthwestsouthwestnorthwestsouthwestsoutheastsoutheastnortheastnortheastnorthwestsoutheastnortheastsouthwestnortheastnortheastnorthwestsoutheastsoutheastsouthwestsoutheastnortheastnorthwestnortheastsoutheastsouthwestnorthwestsoutheastnortheastsoutheastnortheastsoutheastnortheastsouthwestnorthwestsoutheastnorthwestsouthwestsoutheastnorthwestsoutheastnortheastnorthwestnortheastsouthwestsoutheastsouthwestnortheastnortheastsoutheastsoutheastnortheastsouthwestsoutheastsouthwestsouthwestsouthwestsouthwestnortheastnorthwestnorthwestnortheastsoutheastsouthwestnorthwestsouthwestsouthwestsoutheastnortheastsouthwestnortheastnorthwestnortheastnortheastsoutheastsouthwestnorthwestnortheastnorthwestsoutheastnortheastnorthwestnortheastnortheastnortheastsoutheastsoutheastsoutheastsoutheastsoutheastsoutheastsouthwestnorthwestnortheastnortheastsoutheastsoutheastnortheastsoutheastsoutheastsoutheastsouthwestnorthwestnortheastsoutheastsoutheastnortheastsoutheastsoutheastsoutheastnorthwestsoutheastnorthwestsouthwestnorthwestsouthwestsouthwestsouthwestnorthwestsouthwestnortheastnortheastsoutheastsouthwestsoutheastnortheastsouthwestnorthwestnorthwestsouthwestnorthwestnortheastsoutheastnorthwestsoutheastsoutheastnortheastsoutheastnorthwestnorthwestsouthwestsoutheastnorthwestnortheastnortheastsoutheastsoutheastnorthwestnortheastsouthwestsouthwestnorthwestnorthwestnorthwestnorthwestnortheastsouthwestsouthwestsouthwestnortheastsoutheastnorthwestnortheastnortheastsoutheastsoutheastsouthwestsouthwestnortheastsouthwestnorthwestsoutheastsouthwestnortheastsouthwestnortheastsoutheastnortheastsoutheastnorthwestnorthwestsouthwestnorthwestsouthwestnorthwestsoutheastnorthwestnorthwestsoutheastnortheastnortheastnortheastsoutheastnortheastsoutheastnortheastsouthwestsouthwestnortheastnortheastnorthwestsouthwestsouthwestsouthwestnorthwestsouthwestsouthwestnortheastnorthwestnorthwestsoutheastnorthwestnortheastsoutheastnorthwestsouthwestsoutheastsoutheastsouthwestsoutheastsouthwestnortheastnorthwestnortheastsoutheastsoutheastsouthwestnortheastsoutheastnorthwestnorthwestsoutheastnortheastsouthwestnortheastsoutheastsoutheastsouthwestsoutheastsoutheastnorthwestnorthwestnorthwestsoutheastnortheastsouthwestnorthwestsoutheastnortheastsoutheastsoutheastnorthwestsouthwestnortheastnorthwestsouthwestnorthwestsoutheastsoutheastnorthwestnortheastsoutheastnortheastnortheastnorthwestsouthwestnorthwestnorthwestnorthwestnorthwestsoutheastsouthwestnortheastnortheastnorthwestsouthwestnortheastsoutheastsoutheastnortheastsoutheastsouthwestsoutheastsouthwestnorthwestnortheastnorthwestnortheastnortheastnortheastsouthwestsoutheastnorthwestsoutheastsouthwestsouthwestsouthwestnorthwestnortheastsouthwestsouthwestsoutheastsouthwestnortheastsouthwestsouthwestsoutheastsoutheastsouthwestnortheastnorthwestsouthwestsoutheastsouthwestsouthwestsouthwestnorthwestnortheastnortheastnorthwestsoutheastnortheastnorthwestsouthwestnorthwestnorthwestsoutheastnortheastsoutheastnortheastsouthwestnortheastnorthwestsoutheastnorthwestnortheastsoutheastnortheastsoutheastsouthwestnortheastnortheastsoutheastsouthwestsouthwestnorthwestnortheastnorthwestsouthwestnorthwestsoutheastnorthwestnortheastsouthwestsoutheastsouthwestsouthwestsouthwestnorthwestsouthwestnortheastsouthwestsouthwestsoutheastsoutheastsoutheastsouthwestsouthwestsouthwestsoutheastsouthwestnortheastnorthwestsoutheastsoutheastsoutheastsoutheastnortheastnorthwestsoutheastsouthwestsouthwestnortheastnorthwestsouthwestnortheastsoutheastnorthwestsouthwestnorthwestsoutheastsoutheastnorthwestnortheastnorthwestnorthwestsouthwestsoutheastnortheastnorthwestsouthwestnorthwestnorthwestnortheastsoutheastsoutheastnortheastnorthwestsouthwestsoutheastnortheastsoutheastsoutheastnortheastsoutheastnortheastnortheastsouthwestnortheastnortheastsouthwestnorthwestnorthwestnortheastnortheastsouthwestnortheastsouthwestsoutheastnorthwestnortheastsouthwestsoutheastnortheastnorthwestnorthwestsouthwestsouthwestsouthwestnortheastnorthwestnortheastnorthwestnortheastnortheastsouthwestsoutheastsoutheastnorthwestsouthwestnorthwestnorthwestsouthwestnorthwestnorthwestsouthwestsoutheastsoutheastsoutheastsoutheastsouthwestnorthwestnorthwestsouthwestnortheastnorthwestsoutheastnortheastnortheastnorthwestsouthwestsoutheastnorthwestnortheastnorthwestnorthwestnortheastnortheastsouthwestnortheastnorthwestnortheastsoutheastnorthwestsouthwestnorthwestnortheastnortheastsouthwestnorthwestnorthwestsouthwestsoutheastsoutheastnorthwestsoutheastsoutheastsoutheastnorthwestsouthwestsouthwestsoutheastnortheastnorthwestsoutheastsoutheastnortheastnorthwestnortheastnortheastsoutheastsouthwestnortheastsoutheastsoutheastsoutheastnorthwestnorthwestsouthwestnorthwestsouthwestnortheastnorthwestsoutheastsouthwestsoutheastnortheastsouthwestnorthwestsouthwestnortheastnortheastsoutheastnortheastsoutheastnortheastsouthwestsoutheastsoutheastsouthwestsoutheastnorthwestnorthwestsouthwestsoutheastnortheastsoutheastsoutheastnorthwestnortheastsoutheastnortheastsoutheastsoutheastnorthwestsouthwestsoutheastnorthwestnortheastnortheastnorthwestsouthwestsoutheastsouthwestsouthwestsoutheastsouthwestnortheastnorthwestnorthwestnorthwestsouthwestnorthwestsoutheastnorthwestsoutheastsouthwestsoutheastsoutheastsouthwestnorthwestsouthwestnorthwestsouthwestsouthwestnortheastnorthwestsoutheastnorthwestnorthwestnortheastsoutheastnorthwestnortheastsouthwestnorthwestsoutheastsoutheastnortheastnorthwestnortheastsoutheastsouthwestsouthwestnorthwestnortheastsouthwestsoutheastnorthwestnorthwestsouthwestnorthwestsouthwestnortheastsoutheastnortheastnorthwestsouthwestnortheastsoutheastnortheastnorthwestnorthwestnortheastsouthwestnorthwestnortheastnortheastnorthwestnorthwestnorthwestnorthwestsoutheastnorthwestsouthwestnorthwestnorthwestnorthwestnortheastsoutheastnorthwestsouthwestsouthwestnortheastsouthwestnorthwestsoutheastnortheastsouthwestnorthwestnortheastsoutheastsoutheastsouthwestnorthwestnortheastsoutheastsoutheastsoutheastnortheastsoutheastnortheastsoutheastsoutheastnortheastnorthwestsouthwestnorthwestsouthwestsoutheastnorthwestnortheastnorthwestnortheastsoutheastsoutheastsoutheastnorthwestsoutheastsoutheastsouthwestsouthwestsouthwestsoutheastnortheastnortheastsouthwestsouthwestsouthwestsoutheastsouthwestnorthwestnorthwestnorthwestnortheastnortheastsouthwestsoutheastsouthwestnortheastsoutheastsouthwestnortheastsouthwestsouthwestnorthwestnorthwestsoutheastsoutheastsoutheastsouthwestnortheastnorthwestnortheastnorthwestsoutheastnorthwestnortheastsoutheastsouthwestnortheastnortheastsouthwestsouthwestsoutheastnortheastsouthwestsoutheastnorthwestnortheastsouthwestnortheastsoutheastnorthwestnorthwestsoutheastnorthwestsouthwestsouthwestnortheastsoutheastnortheastnorthwestsouthwestsouthwestnorthwestnorthwestsouthwestsouthwestnorthwestnortheastsouthwestsoutheastnortheastnorthwestnorthwestnortheastsoutheastsoutheastnorthwestnortheastnortheastsoutheastnortheastsouthwestsoutheastsouthwestsouthwestnorthwestnortheastsoutheastsouthwestnorthwest'] to numeric\n",
      "\n",
      "Full API response: {'id': 'gen-MogYQHFAij8mws698nv6nQneC6wA', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324335, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the corrected code snippet:\\n\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\ndef step_31(df):\\n    df_cleaned = df.copy()\\n    data_types_info = df_cleaned.dtypes.apply(lambda x: x.name).to_dict()\\n    \\n    # Identify missing values\\n    missing_values = df_cleaned.isnull().sum()\\n    \\n    # Handle missing values\\n    for column in df_cleaned.columns:\\n        if df_cleaned[column].dtype in ['object']:\\n            df_cleaned[column].fillna(df_cleaned[column].mode()[0], inplace=True)\\n        else:\\n            df_cleaned[column].fillna(df_cleaned[column].mean(), inplace=True)\\n    \\n    return df_cleaned, data_types_info\\n```\"}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 4909, 'completion_tokens': 152, 'total_tokens': 5061}}\n",
      "Full API response: {'id': 'gen-KAtV24ZnzNABN4QQyJvZmqw4T0L8', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324340, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the function definition:\\n\\n```\\nimport pandas as pd\\nfrom sklearn.preprocessing import LabelEncoder\\n\\ndef step_32(df_cleaned, data_types_info):\\n    df_encoded = df_cleaned.copy()\\n    categorical_cols = [col for col, dtype in data_types_info.items() if dtype == 'object']\\n    le = LabelEncoder()\\n    for col in categorical_cols:\\n        df_encoded[col] = le.fit_transform(df_encoded[col])\\n    return df_encoded\\n```\"}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 1002, 'completion_tokens': 96, 'total_tokens': 1098}}\n",
      "Full API response: {'id': 'gen-kiyBukbldBRpPgaPZNm5NSzZXuEF', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324346, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the function definition for `step_51`:\\n\\n```\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\n\\ndef step_51(df_encoded):\\n    X = df_encoded.drop('charges', axis=1)\\n    y = df_encoded['charges']\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n    return model, X_train, X_test, y_train, y_test\\n```\"}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 1024, 'completion_tokens': 129, 'total_tokens': 1153}}\n",
      "Full API response: {'id': 'gen-BYusMLW6ZfqH5KaPh38sfDsFbZBF', 'model': 'meta-llama/llama-3-70b-instruct', 'object': 'chat.completion', 'created': 1721324354, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here is the Python function named 'step_61':\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error\\n\\ndef step_61(model, X_train, X_test, y_train, y_test):\\n    y_train_pred = model.predict(X_train)\\n    y_test_pred = model.predict(X_test)\\n\\n    if y_train.dtype.kind in ['i', 'f']:\\n        metrics = ['R^2', 'MSE', 'RMSE']\\n        evaluation_results = {\\n            'train': {'R^2': r2_score(y_train, y_train_pred), 'MSE': mean_squared_error(y_train, y_train_pred), 'RMSE': mean_squared_error(y_train, y_train_pred, squared=False)},\\n            'test': {'R^2': r2_score(y_test, y_test_pred), 'MSE': mean_squared_error(y_test, y_test_pred), 'RMSE': mean_squared_error(y_test, y_test_pred, squared=False)}\\n        }\\n    else:\\n        metrics = ['accuracy', 'precision', 'recall', 'F1-score']\\n        evaluation_results = {\\n            'train': {'accuracy': accuracy_score(y_train, y_train_pred), 'precision': precision_score(y_train, y_train_pred, average='weighted'), 'recall': recall_score(y_train, y_train_pred, average='weighted'), 'F1-score': f1_score(y_train, y_train_pred, average='weighted')},\\n            'test': {'accuracy': accuracy_score(y_test, y_test_pred), 'precision': precision_score(y_test, y_test_pred, average='weighted'), 'recall': recall_score(y_test, y_test_pred, average='weighted'), 'F1-score': f1_score(y_test, y_test_pred, average='weighted')}\\n        }\\n\\n    return evaluation_results, metrics\\n```\"}, 'finish_reason': 'stop', 'logprobs': {'tokens': None, 'token_logprobs': None, 'top_logprobs': None, 'text_offset': None}}], 'usage': {'prompt_tokens': 1037, 'completion_tokens': 389, 'total_tokens': 1426}}\n",
      "Validation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "\n",
    "# Configure logger\n",
    "logger.add(\"execution.log\", rotation=\"500 MB\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_url = \"https://openrouter.ai/api/v1\"\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "# Example step and validation scripts to guide the model\n",
    "example_step_script = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def step_32(df_cleaned, data_types_info):\n",
    "    df_encoded = df_cleaned.copy()\n",
    "    categorical_cols = [col for col, dtype in data_types_info.items() if dtype == 'object']\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "    return df_encoded\n",
    "\"\"\"\n",
    "\n",
    "example_validation_script = \"\"\"\n",
    "import pandas as pd\n",
    "from step_11 import step_11\n",
    "from step_31 import step_31\n",
    "from step_32 import step_32\n",
    "\n",
    "def validate_step():\n",
    "    csv_path = '/Users/ilya/Desktop/GitHub_Repositories/HW_University/Data_Mining/datasets/insurance.csv'\n",
    "    df = step_11(csv_path)\n",
    "    df_cleaned, data_types_info = step_31(df)\n",
    "    df_encoded = step_32(df_cleaned, data_types_info)\n",
    "    print(df_encoded)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    validate_step()\n",
    "\"\"\"\n",
    "\n",
    "def openai_chat(request):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"meta-llama/llama-3-70b-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": request}]\n",
    "    }\n",
    "    response = requests.post(f\"{api_url}/chat/completions\", headers=headers, json=data)\n",
    "    \n",
    "    # Log the full response for debugging\n",
    "    response_json = response.json()\n",
    "    print(\"Full API response:\", response_json)\n",
    "    \n",
    "    # Check if 'choices' key exists in the response\n",
    "    if \"choices\" in response_json and response_json[\"choices\"]:\n",
    "        return response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        print(\"Request:\", data)\n",
    "        print(\"Response:\", response_json)\n",
    "        raise ValueError(\"The response does not contain 'choices'. Full response: \" + str(response_json))\n",
    "\n",
    "# # Test the function with a sample request to see the full response\n",
    "# try:\n",
    "#     result = openai_chat(\"Write a Python function that adds two numbers.\")\n",
    "#     print(\"Result:\", result)\n",
    "# except Exception as e:\n",
    "#     print(\"Error:\", e)\n",
    "\n",
    "\n",
    "def generate_code_snippet(request):\n",
    "    response = openai_chat(request)\n",
    "    return response\n",
    "\n",
    "def clean_and_correct_code(generated_code, csv_path):\n",
    "    cleaned_code = generated_code.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    cleaned_code_lines = [line for line in cleaned_code.split(\"\\n\") if not line.lower().startswith(\"here is the\")]\n",
    "    cleaned_code = \"\\n\".join(cleaned_code_lines)\n",
    "    corrected_code = cleaned_code.replace(\"{csv_path}\", f\"'{csv_path}'\")\n",
    "    return corrected_code\n",
    "\n",
    "def validate_unit_code(code_filename):\n",
    "    try:\n",
    "        result = subprocess.run([\"python\", code_filename], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(result.stderr)\n",
    "        return True, result.stdout\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def fix_code(code_snippet, error_message, csv_path):\n",
    "    request = (\n",
    "        f\"The following code snippet encountered an error:\\n\\n{code_snippet}\\n\\n\"\n",
    "        f\"Error message:\\n{error_message}\\n\\n\"\n",
    "        f\"Please fix the code snippet to resolve the error without providing any explanations or comments.\"\n",
    "    )\n",
    "    fixed_code = generate_code_snippet(request)\n",
    "    return clean_and_correct_code(fixed_code, csv_path)\n",
    "\n",
    "def get_all_dependencies(step, workflow_steps):\n",
    "    dependencies = set(workflow_steps[step][\"dependencies\"])\n",
    "    for dep in workflow_steps[step][\"dependencies\"]:\n",
    "        dependencies.update(get_all_dependencies(dep, workflow_steps))\n",
    "    return dependencies\n",
    "\n",
    "def generate_code_for_step(step, workflow_steps, csv_path, dataset_info):\n",
    "    request = (\n",
    "        f\"Here is an example of a good step script:\\n\\n{example_step_script}\\n\\n\"\n",
    "        f\"Write a Python function named 'step_{step}' for the following step: {workflow_steps[step]['description']}. \"\n",
    "        f\"The function should take {', '.join(workflow_steps[step]['input'])} as input and return {', '.join(workflow_steps[step]['output'])}. \"\n",
    "        f\"Ensure to include necessary imports and handle edge cases. \"\n",
    "        f\"The dataset has the following columns: {dataset_info['columns']}. \"\n",
    "        f\"The data types are: {dataset_info['types']}. \"\n",
    "        f\"Here's a sample of the data: {dataset_info['sample_data']}. \"\n",
    "        f\"Value counts (top 5): {dataset_info['value_counts']}. \"\n",
    "        f\"Statistical description: {dataset_info['description']}. \"\n",
    "        f\"Only return the function definition without any additional code or explanations.\"\n",
    "    )\n",
    "    code_snippet = generate_code_snippet(request)\n",
    "    return clean_and_correct_code(code_snippet, csv_path)\n",
    "\n",
    "def generate_validation_file(step, workflow_steps):\n",
    "    dependencies = get_all_dependencies(step, workflow_steps)\n",
    "    input_params = workflow_steps[step][\"input\"]\n",
    "    output_params = workflow_steps[step][\"output\"]\n",
    "\n",
    "    validation_code = \"import pandas as pd\\n\"\n",
    "    for dep in sorted(dependencies):\n",
    "        validation_code += f\"from step_{dep} import step_{dep}\\n\"\n",
    "    validation_code += f\"from step_{step} import step_{step}\\n\\n\"\n",
    "\n",
    "    validation_code += \"def validate_step():\\n\"\n",
    "    validation_code += \"    csv_path = '/Users/ilya/Desktop/GitHub_Repositories/Thesis/datasets/insurance.csv'\\n\"\n",
    "\n",
    "    for dep in sorted(dependencies):\n",
    "        dep_inputs = \", \".join(workflow_steps[dep][\"input\"])\n",
    "        dep_outputs = \", \".join(workflow_steps[dep][\"output\"])\n",
    "        validation_code += f\"    {dep_outputs} = step_{dep}({dep_inputs})\\n\"\n",
    "\n",
    "    input_values = \", \".join(input_params)\n",
    "    output_values = \", \".join(output_params)\n",
    "    validation_code += f\"    {output_values} = step_{step}({input_values})\\n\"\n",
    "    validation_code += f\"    print({output_values})\\n\"\n",
    "\n",
    "    validation_code += \"\\nif __name__ == '__main__':\\n\"\n",
    "    validation_code += \"    validate_step()\\n\"\n",
    "\n",
    "    with open(f\"validate_step_{step}.py\", \"w\") as file:\n",
    "        file.write(validation_code)\n",
    "\n",
    "def save_dataset_info(csv_path, info_file_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    columns = df.columns.tolist()\n",
    "    types = df.dtypes.apply(lambda x: str(x)).to_dict()\n",
    "    sample_data = df.head().to_dict(orient='list')\n",
    "    value_counts = {col: df[col].value_counts().head().to_dict() for col in df.columns}\n",
    "    description = df.describe().to_dict()\n",
    "\n",
    "    dataset_info = {\n",
    "        'columns': columns,\n",
    "        'types': types,\n",
    "        'sample_data': sample_data,\n",
    "        'value_counts': value_counts,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "    with open(info_file_path, 'w') as file:\n",
    "        json.dump(dataset_info, file)\n",
    "\n",
    "def main():\n",
    "    csv_path = \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/datasets/insurance.csv\"\n",
    "    info_file_path = \"dataset_info.json\"\n",
    "\n",
    "    if not os.path.exists(info_file_path):\n",
    "        save_dataset_info(csv_path, info_file_path)\n",
    "\n",
    "    with open(info_file_path, 'r') as f:\n",
    "        dataset_info = json.load(f)\n",
    "\n",
    "    selected_step_numbers = [11, 21, 31, 32, 51, 61]\n",
    "    for step in selected_step_numbers:\n",
    "        try:\n",
    "            code_snippet = generate_code_for_step(step, workflow_steps, csv_path, dataset_info)\n",
    "            with open(f\"step_{step}.py\", \"w\") as file:\n",
    "                file.write(code_snippet)\n",
    "            generate_validation_file(step, workflow_steps)\n",
    "\n",
    "            success, output = validate_unit_code(f\"validate_step_{step}.py\")\n",
    "            while not success:\n",
    "                print(f\"Validation failed for step {step}: {output}\")\n",
    "                fixed_code = fix_code(code_snippet, output, csv_path)\n",
    "                with open(f\"step_{step}.py\", \"w\") as file:\n",
    "                    file.write(fixed_code)\n",
    "                generate_validation_file(step, workflow_steps)\n",
    "                success, output = validate_unit_code(f\"validate_step_{step}.py\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing step {step}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"Validation completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the Main Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main script generated successfully.\n",
      "Main script validated successfully.\n"
     ]
    }
   ],
   "source": [
    "def generate_main_file(workflow_steps, selected_step_numbers, csv_path):\n",
    "    main_code = \"import pandas as pd\\n\\n\"\n",
    "    for step in selected_step_numbers:\n",
    "        main_code += f\"from step_{step} import step_{step}\\n\"\n",
    "\n",
    "    main_code += \"\\ndef main():\\n\"\n",
    "    main_code += f\"    csv_path = '{csv_path}'\\n\"\n",
    "    main_code += \"    df = pd.read_csv(csv_path)\\n\"\n",
    "\n",
    "    for step in selected_step_numbers:\n",
    "        input_params = \", \".join(workflow_steps[step]['input'])\n",
    "        output_params = \", \".join(workflow_steps[step]['output'])\n",
    "        main_code += f\"    {output_params} = step_{step}({input_params})\\n\"\n",
    "\n",
    "    main_code += \"\\nif __name__ == '__main__':\\n\"\n",
    "    main_code += \"    main()\"\n",
    "\n",
    "    with open(\"main.py\", \"w\") as file:\n",
    "        file.write(main_code)\n",
    "\n",
    "def main():\n",
    "    csv_path = \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/datasets/insurance.csv\"\n",
    "    info_file_path = \"dataset_info.json\"\n",
    "\n",
    "    if not os.path.exists(info_file_path):\n",
    "        save_dataset_info(csv_path, info_file_path)\n",
    "\n",
    "    with open(info_file_path, 'r') as f:\n",
    "        dataset_info = json.load(f)\n",
    "\n",
    "    selected_step_numbers = [11, 21, 31, 32, 51, 61]\n",
    "    \n",
    "    generate_main_file(workflow_steps, selected_step_numbers, csv_path)\n",
    "\n",
    "    print(\"Main script generated successfully.\")\n",
    "\n",
    "    # Validate the main script\n",
    "    success, output = validate_unit_code(\"main.py\")\n",
    "    if success:\n",
    "        print(\"Main script validated successfully.\")\n",
    "    else:\n",
    "        print(f\"Validation failed for main script.\")\n",
    "        print(f\"Error: {output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

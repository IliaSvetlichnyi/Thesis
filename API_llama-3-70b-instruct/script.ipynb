{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the workflow steps with assigned numbers\n",
    "workflow_steps = {\n",
    "    11: \"Load the CSV file into a suitable format (e.g., DataFrame)\",\n",
    "    21: \"Examine the structure and characteristics of the data\",\n",
    "    22: \"Identify missing values, data types, and statistical summary\",\n",
    "    23: \"Visualize the data using charts, graphs, or plots\",\n",
    "    24: \"Gain insights and formulate hypotheses\",\n",
    "    31: \"Handle missing values (remove or impute)\",\n",
    "    32: \"Convert categorical variables to numerical representations\",\n",
    "    33: \"Perform feature scaling or normalization\",\n",
    "    34: \"Encode categorical variables (one-hot encoding, label encoding, etc.)\",\n",
    "    35: \"Split the data into training and testing sets\",\n",
    "    41: \"Create new features based on domain knowledge or data insights\",\n",
    "    42: \"Combine or transform existing features\",\n",
    "    43: \"Perform feature selection to identify relevant features\",\n",
    "    51: \"Choose appropriate machine learning algorithms based on the problem type\",\n",
    "    52: \"Define the model architecture and hyperparameters\",\n",
    "    53: \"Train the selected model on the training data\",\n",
    "    54: \"Utilize techniques like cross-validation for model evaluation\",\n",
    "    61: \"Evaluate the trained model's performance on the testing data\",\n",
    "    62: \"Calculate evaluation metrics (e.g., accuracy, precision, recall, F1-score)\",\n",
    "    63: \"Visualize the model's performance using confusion matrix, ROC curve, etc.\",\n",
    "    64: \"Fine-tune the model if necessary\",\n",
    "    71: \"Analyze the model's coefficients or feature importances\",\n",
    "    72: \"Visualize the model's decision boundaries or learned patterns\",\n",
    "    73: \"Interpret the model's predictions and explain its behavior\",\n",
    "    81: \"Generate unit code documentation during the code generation process\",\n",
    "    82: \"Execute the combined code and capture relevant outputs and insights\",\n",
    "    83: \"Create a comprehensive documentation for the entire workflow, including project overview, dataset details, selected steps, results, and interpretations\",\n",
    "    84: \"Present the documentation to users for understanding and reference\"\n",
    "}\n",
    "\n",
    "# OpenAI API configuration\n",
    "api_url = \"https://openrouter.ai/api/v1\"\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "\n",
    "def openai_chat(request):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"meta-llama/llama-3-70b-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": request}]\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/chat/completions\", headers=headers, json=data)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed for step: Handle missing values (remove or impute)\n",
      "Error:   File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct/step_4_code.py\", line 30\n",
      "    Note that you need to specify the columns to impute and the strategy for imputation. In this example, I'm imputing missing values in the numerical columns with the mean.\n",
      "                                                                                                           ^\n",
      "SyntaxError: unterminated string literal (detected at line 30)\n",
      "\n",
      "Validation failed for step: Convert categorical variables to numerical representations\n",
      "Error:   File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct/step_5_code.py\", line 13\n",
      "    This code uses the `pd.get_dummies()` function to convert the categorical variables `sex`, `smoker`, and `region` into numerical representations using one-hot encoding. The resulting dataframe will have additional columns for each category in each categorical variable.\n",
      "         ^^^^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "Validation failed for step: Choose appropriate machine learning algorithms based on the problem type\n",
      "Error:   File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct/step_7_code.py\", line 33\n",
      "    This code loads the dataset, preprocesses the data by one-hot encoding the categorical features, splits the data into training and testing sets, and defines three machine learning algorithms: Linear Regression, Decision Tree Regressor, and Random Forest Regressor. It then trains and evaluates each algorithm using the mean squared error metric.\n",
      "         ^^^^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "Validation failed for step: Define the model architecture and hyperparameters\n",
      "Error:   File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct/step_8_code.py\", line 36\n",
      "    This code assumes that the dataset is stored in a CSV file at the specified path. It loads the dataset, preprocesses the data by one-hot encoding categorical variables, splits the data into training and testing sets, and defines three models with their respective hyperparameters. The models are then trained and evaluated using the mean squared error (MSE) metric.\n",
      "         ^^^^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "Validation failed for step: Evaluate the trained model's performance on the testing data\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"/Users/ilya/Desktop/GitHub_Repositories/Thesis/API_llama-3-70b-instruct/step_10_code.py\", line 8, in <module>\n",
      "    dataset = pd.read_csv('/Users/ilya/Desktop/GitHub_Repositories/HW_University/Data_Mining/datasets/insurance.csv')\n",
      "NameError: name 'pd' is not defined. Did you mean: 'id'?\n",
      "\n",
      "Documentation saved to documentation.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to generate code with OpenAI API\n",
    "\n",
    "\n",
    "def generate_code_snippet(request):\n",
    "    response = openai_chat(request)\n",
    "    return response\n",
    "\n",
    "# Function to generate code with placeholder data\n",
    "\n",
    "\n",
    "def generate_code_snippet_with_placeholder(request, placeholder_data):\n",
    "    placeholder_request = request + \\\n",
    "        f\"\\nUse the following placeholder data:\\n{placeholder_data}\"\n",
    "    response = openai_chat(placeholder_request)\n",
    "    return response\n",
    "\n",
    "# Function to clean and correct the code\n",
    "\n",
    "\n",
    "def clean_and_correct_code(generated_code, csv_path):\n",
    "    cleaned_code = generated_code.replace(\"```\", \"\").strip()\n",
    "    cleaned_code_lines = cleaned_code.split(\"\\n\")\n",
    "    cleaned_code_lines = [\n",
    "        line for line in cleaned_code_lines if not line.lower().startswith(\"here is the\")]\n",
    "    cleaned_code = \"\\n\".join(cleaned_code_lines)\n",
    "    if \"python\" in cleaned_code:\n",
    "        cleaned_code = cleaned_code.split(\"python\")[1].strip()\n",
    "    corrected_code = cleaned_code.replace(\"{csv_path}\", f\"{csv_path}\")\n",
    "    return corrected_code\n",
    "\n",
    "# Load dataset information\n",
    "\n",
    "\n",
    "def get_dataset_info(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    columns = df.columns.tolist()\n",
    "    types = df.dtypes.to_dict()\n",
    "    sample_data = df.head().to_dict(orient='list')\n",
    "    value_counts = {col: df[col].value_counts().to_dict()\n",
    "                    for col in df.columns}\n",
    "    description = df.describe().to_dict()\n",
    "    return columns, types, sample_data, value_counts, description\n",
    "\n",
    "# Function to get selected workflow steps based on step numbers\n",
    "\n",
    "\n",
    "def get_selected_steps(step_numbers):\n",
    "    selected_steps = [workflow_steps[num]\n",
    "                      for num in step_numbers if num in workflow_steps]\n",
    "    return selected_steps\n",
    "\n",
    "# Function to validate a single unit code snippet\n",
    "\n",
    "\n",
    "def validate_unit_code(code_filename):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", code_filename], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(result.stderr)\n",
    "        return True, result.stdout\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Function to generate documentation for a step\n",
    "\n",
    "\n",
    "def generate_documentation(step, columns_info, types_info, sample_data_info, value_counts_info, description_info):\n",
    "    request = (\n",
    "        f\"Provide a clear and concise description of the job performed by the code for the following step: {step}. \"\n",
    "        f\"The description should summarize the main tasks and key points without going into the specifics of the code. \"\n",
    "        f\"The dataset has the following columns: {columns_info}. \"\n",
    "        f\"The data types are: {types_info}. Sample data: {sample_data_info}. Value counts: {value_counts_info}. \"\n",
    "        f\"Description: {description_info}.\"\n",
    "    )\n",
    "    documentation = generate_code_snippet(request)\n",
    "    return documentation\n",
    "\n",
    "# Function to fix the code based on the error\n",
    "\n",
    "\n",
    "def fix_code(code_snippet, error_message, csv_path):\n",
    "    request = (\n",
    "        f\"The following code snippet encountered an error:\\n\\n{code_snippet}\\n\\n\"\n",
    "        f\"Error message:\\n{error_message}\\n\\n\"\n",
    "        f\"Please fix the code snippet to resolve the error without providing any explanations or comments.\"\n",
    "    )\n",
    "    fixed_code = generate_code_snippet(request)\n",
    "    cleaned_fixed_code = clean_and_correct_code(fixed_code, csv_path)\n",
    "    return cleaned_fixed_code\n",
    "\n",
    "# Main function\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_path = \"/Users/ilya/Desktop/GitHub_Repositories/HW_University/Data_Mining/datasets/insurance.csv\"\n",
    "    columns, types, sample_data, value_counts, description = get_dataset_info(\n",
    "        csv_path)\n",
    "\n",
    "    columns_info = \", \".join(columns)\n",
    "    types_info = \", \".join([f\"{col}: {typ}\" for col, typ in types.items()])\n",
    "    sample_data_info = \", \".join(\n",
    "        [f\"{col}: {vals[:5]}\" for col, vals in sample_data.items()])\n",
    "    value_counts_info = \", \".join(\n",
    "        [f\"{col}: {dict(list(vc.items())[:5])}\" for col, vc in value_counts.items()])\n",
    "    description_info = \", \".join(\n",
    "        [f\"{col}: {desc}\" for col, desc in description.items()])\n",
    "\n",
    "    # Hardcode the selected step numbers (replace with dynamic selection later)\n",
    "    selected_step_numbers = [11, 21, 22, 31, 32, 35, 51, 52, 53, 61, 62]\n",
    "\n",
    "    # Get selected workflow steps based on step numbers\n",
    "    selected_steps = get_selected_steps(selected_step_numbers)\n",
    "\n",
    "    # Generate and validate code snippets for each selected step\n",
    "    unit_code_filenames = []\n",
    "    documentation_snippets = []\n",
    "    combined_code = \"\"\n",
    "    placeholder_data = \"\"\n",
    "    for i, step in enumerate(selected_steps):\n",
    "        request = (\n",
    "            f\"Write a Python code snippet for the following step: {step}. \"\n",
    "            f\"Use placeholders like {csv_path} for dynamic inputs. The dataset has the following columns: {columns_info}. \"\n",
    "            f\"The data types are: {types_info}. Sample data: {sample_data_info}. Value counts: {value_counts_info}. \"\n",
    "            f\"Description: {description_info}. Only return the code without any explanations.\"\n",
    "        )\n",
    "\n",
    "        if i < len(selected_steps) - 1:\n",
    "            if placeholder_data:\n",
    "                code_snippet = generate_code_snippet_with_placeholder(\n",
    "                    request, placeholder_data)\n",
    "            else:\n",
    "                code_snippet = generate_code_snippet(request)\n",
    "\n",
    "            cleaned_code_snippet = clean_and_correct_code(\n",
    "                code_snippet, csv_path)\n",
    "\n",
    "            code_filename = f\"step_{i+1}_code.py\"\n",
    "            with open(code_filename, \"w\") as file:\n",
    "                file.write(cleaned_code_snippet)\n",
    "\n",
    "            fixed_code_snippet = cleaned_code_snippet  # Initialize fixed_code_snippet\n",
    "            success, output = validate_unit_code(code_filename)\n",
    "            while not success:\n",
    "                print(f\"Validation failed for step: {step}\")\n",
    "                print(f\"Error: {output}\")\n",
    "                fixed_code_snippet = fix_code(\n",
    "                    cleaned_code_snippet, output, csv_path)\n",
    "                with open(code_filename, \"w\") as file:\n",
    "                    file.write(fixed_code_snippet)\n",
    "                success, output = validate_unit_code(code_filename)\n",
    "\n",
    "            placeholder_data += output + \"\\n\"  # Append the output to placeholder data\n",
    "\n",
    "            unit_code_filenames.append(code_filename)\n",
    "            documentation_snippet = generate_documentation(\n",
    "                step, columns_info, types_info, sample_data_info, value_counts_info, description_info)\n",
    "            documentation_snippets.append(documentation_snippet)\n",
    "            combined_code += fixed_code_snippet + \"\\n\\n\"\n",
    "        # Combine code snippets and validate for the last step (model training and evaluation)\n",
    "        else:\n",
    "            cleaned_code_snippet = clean_and_correct_code(\n",
    "                code_snippet, csv_path)\n",
    "            fixed_combined_code = cleaned_code_snippet  # Initialize fixed_combined_code\n",
    "            combined_code += cleaned_code_snippet + \"\\n\\n\"\n",
    "            with open(\"combined_code.py\", \"w\") as file:\n",
    "                file.write(combined_code)\n",
    "\n",
    "            success, output = validate_unit_code(\"combined_code.py\")\n",
    "            while not success:\n",
    "                print(f\"Validation failed for step: {step}\")\n",
    "                print(f\"Error: {output}\")\n",
    "                fixed_combined_code = fix_code(combined_code, output, csv_path)\n",
    "                with open(\"combined_code.py\", \"w\") as file:\n",
    "                    file.write(fixed_combined_code)\n",
    "                success, output = validate_unit_code(\"combined_code.py\")\n",
    "\n",
    "            if success:\n",
    "                unit_code_filenames.append(code_filename)\n",
    "                documentation_snippet = generate_documentation(\n",
    "                    step, columns_info, types_info, sample_data_info, value_counts_info, description_info)\n",
    "                documentation_snippets.append(documentation_snippet)\n",
    "            else:\n",
    "                print(f\"Validation failed for step: {step}\")\n",
    "                print(f\"Error: {output}\")\n",
    "                break\n",
    "\n",
    "    # Save documentation to a separate file if all steps are validated successfully\n",
    "    if len(unit_code_filenames) == len(selected_steps):\n",
    "        with open(\"documentation.txt\", \"w\") as file:\n",
    "            file.write(\"\\n\".join(documentation_snippets))\n",
    "        print(\"Documentation saved to documentation.txt\")\n",
    "    else:\n",
    "        print(\"Some code snippets failed validation. Documentation not generated.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
